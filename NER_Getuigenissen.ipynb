{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER-Getuigenissen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LNXW20DNbx3"
      },
      "source": [
        "# Objective\r\n",
        "\r\n",
        "Use data collected during the Getuigenissen project (https://www.getuigenissen.org). These data contain transcribed texts of police records from the 18th-19th century which were manually enriched with named-entity tags and relationships between the different entities. \r\n",
        "\r\n",
        "- Build a Named Entity Recognition model\r\n",
        "- See how good such a model works\r\n",
        "\r\n",
        "Final objective: apply the named entity recognition model on transcribed images.\r\n",
        "\r\n",
        "# Data\r\n",
        "\r\n",
        "- About 6500 images of 3200 police interrogations covering more than 260 police cases were transcribed 2 times by volunteers.\r\n",
        "- Some of these transcriptions were manually checked by researchers, for others, the transcription of the best transcriber was taken.\r\n",
        "- For each of these 260 police records 1 text was sampled and was manually annotated with named entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh4eMr2UT53e"
      },
      "source": [
        "# Data analysis setup\r\n",
        "\r\n",
        "1. Finetune BERT models (multilingual BERT, RobBERT https://github.com/iPieter/RobBERT, BERTje https://github.com/wietsedv/bertje) on the data and investigate accuracies of the named entity recognition task\r\n",
        "2. Compare to building and finetuning a Conditional Random Field\r\n",
        "3. Score the model on not annotated data\r\n",
        "\r\n",
        "Notes\r\n",
        "\r\n",
        "*   due to the nature of the police reports of the 18th-19th century, some smaller parts of the texts are in French\r\n",
        "*   unfortunately data is currently not available as open data, but this might change in the future.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWZMhvaAu_HY"
      },
      "source": [
        "# Software installations\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-E5FiAE0agG",
        "outputId": "8ac3c618-0cb0-4468-9bb9-24a442b7c522"
      },
      "source": [
        "%cd /content\r\n",
        "!git clone https://github.com/UniversalDependencies/UD_Dutch-LassySmall\r\n",
        "%cd /content\r\n",
        "!git clone https://github.com/wietsedv/bertje\r\n",
        "%cd bertje"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'UD_Dutch-LassySmall'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 224 (delta 12), reused 11 (delta 4), pack-reused 200\u001b[K\n",
            "Receiving objects: 100% (224/224), 7.39 MiB | 24.17 MiB/s, done.\n",
            "Resolving deltas: 100% (134/134), done.\n",
            "/content\n",
            "Cloning into 'bertje'...\n",
            "remote: Enumerating objects: 166, done.\u001b[K\n",
            "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 166 (delta 37), reused 106 (delta 11), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (166/166), 215.08 KiB | 7.17 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n",
            "/content/bertje\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_oN7Hsvh8l"
      },
      "source": [
        "## NER based on bertje\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCvjAL0-x8Wa"
      },
      "source": [
        "- Check setup of bertje based on UD Lassy Small\r\n",
        "- Input requires 3 files train.tsv, dev.tsv and test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJzcRfX83NxW",
        "outputId": "b440f4d9-ee1d-424a-e7ab-ef5134f674b2"
      },
      "source": [
        "!python --version\r\n",
        "!pip install transformers\r\n",
        "!pip install pyyaml\r\n",
        "!pip install scikit-learn\r\n",
        "!pip install scipy\r\n",
        "!pip install tqdm\r\n",
        "!pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 30.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 19.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=471beef661ed1874c9dea70b79ee2bc13f1c49b1a887028391caf767d4b43ea5\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.19.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.17.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.15.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.7.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.19.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.36.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.2.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard) (3.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrkchdQya6uX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568f5ab4-6ae6-4029-bd99-d50853466201"
      },
      "source": [
        "!python /content/bertje/finetuning/prepare/prepare-ud.py -i \"/content/UD_Dutch-LassySmall\" -o \"data\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " > Preparing NER data\n",
            "Labels in data/pos/train.tsv (16 labels):\n",
            "noun          12325 (16.40%)\n",
            "punct         11295 (15.03%)\n",
            "propn         10559 (14.05%)\n",
            "adp            9293 (12.36%)\n",
            "det            8130 (10.82%)\n",
            "adj            5361 (7.13%)\n",
            "verb           5170 (6.88%)\n",
            "adv            2703 (3.60%)\n",
            "num            2586 (3.44%)\n",
            "pron           2368 (3.15%)\n",
            "cconj          2010 (2.67%)\n",
            "aux            1949 (2.59%)\n",
            "sym             545 (0.73%)\n",
            "sconj           486 (0.65%)\n",
            "x               379 (0.50%)\n",
            "intj              6 (0.01%)\n",
            "\n",
            "Labels in data/pos/dev.tsv (16 labels):\n",
            "noun           1830 (16.03%)\n",
            "punct          1810 (15.85%)\n",
            "adp            1374 (12.03%)\n",
            "propn          1207 (10.57%)\n",
            "det            1173 (10.27%)\n",
            "verb            881 (7.72%)\n",
            "adj             752 (6.59%)\n",
            "pron            565 (4.95%)\n",
            "adv             535 (4.69%)\n",
            "num             386 (3.38%)\n",
            "aux             352 (3.08%)\n",
            "cconj           332 (2.91%)\n",
            "sconj           111 (0.97%)\n",
            "sym              64 (0.56%)\n",
            "x                43 (0.38%)\n",
            "intj              2 (0.02%)\n",
            "\n",
            "Labels in data/pos/test.tsv (15 labels):\n",
            "noun           2087 (18.02%)\n",
            "propn          1625 (14.03%)\n",
            "punct          1558 (13.45%)\n",
            "adp            1486 (12.83%)\n",
            "det            1286 (11.10%)\n",
            "adj             947 (8.18%)\n",
            "verb            721 (6.23%)\n",
            "num             432 (3.73%)\n",
            "adv             381 (3.29%)\n",
            "cconj           360 (3.11%)\n",
            "aux             283 (2.44%)\n",
            "pron            220 (1.90%)\n",
            "sconj            86 (0.74%)\n",
            "sym              66 (0.57%)\n",
            "x                43 (0.37%)\n",
            "\n",
            "NER: Train=0.79, Dev=0.09, Test=0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isMJnmHy5IPj"
      },
      "source": [
        "!mkdir /content/getuigenissen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNsWTG8UyWCP"
      },
      "source": [
        "- Build these 3 files (train.tsv, test.tsv, dev.tsv) locally and upload to Google Colab in the `/content/getuigenissen` folder\r\n",
        "- Upload getuigenissen-ner.yaml to the `bertje/finetuning/v2/configs/data` folder\r\n",
        "\r\n",
        "```\r\n",
        "data:\r\n",
        "  name: \"getuigenissen-ner\"\r\n",
        "  input: \"/content/getuigenissen\"\r\n",
        "  num_labels: 25\r\n",
        "\r\n",
        "model:\r\n",
        "  shortname: \"bertje\"\r\n",
        "  name: \"wietsedv/bert-base-dutch-cased\"\r\n",
        "  type: \"bert\"\r\n",
        "\r\n",
        "train:\r\n",
        "  max_epochs: 200\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U5qqfuNqcdu",
        "outputId": "30524963-feba-447e-de95-f4838668c937"
      },
      "source": [
        "%cd /content/bertje/finetuning/v2\r\n",
        "!python main.py data/getuigenissen-ner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bertje/finetuning/v2\n",
            "2020-12-22 10:55:50.030758: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "importing config from \"configs/default.yaml\"\n",
            "importing config from \"configs/data/getuigenissen-ner.yaml\"\n",
            "data:\n",
            "  cache: cache/{}-{}\n",
            "  cfgs: [data/udlassy-pos, data/lassysmall-pos, data/conll2002-ner, data/sonar-ner,\n",
            "    data/udlassy-ner, data/110kdbrd, data/110kdbrd-2, data/twisty, data/twisty2, data/twisty3,\n",
            "    data/twisty-merge-4, data/twisty4-merge-4]\n",
            "  clip_start: false\n",
            "  dev: true\n",
            "  input: /content/getuigenissen\n",
            "  logs: logs/{}-{}\n",
            "  merge: null\n",
            "  name: getuigenissen-ner\n",
            "  num_labels: 25\n",
            "  num_sents: 1\n",
            "  output: output/{}-{}\n",
            "  token_level: true\n",
            "  verify: false\n",
            "eval: {batch_size: 64}\n",
            "force: false\n",
            "model:\n",
            "  cfgs: [models/bertje, models/multi, models/bertnl, models/robbert]\n",
            "  checkpoint: -1\n",
            "  device: cuda\n",
            "  do_export: true\n",
            "  do_train: true\n",
            "  lower_case: false\n",
            "  name: wietsedv/bert-base-dutch-cased\n",
            "  shortname: bertje\n",
            "  type: bert\n",
            "optimizer: {adam_epsilon: 1.0e-08, learning_rate: 5.0e-05, max_grad_norm: 1.0, warmup_steps: 512,\n",
            "  weight_decay: 0.05}\n",
            "summary: {groups: false, method: accuracy, probs: false, type: dev}\n",
            "train: {attention_dropout: 0.2, batch_size: 6, eval_steps: 0.25, gradient_accumulation_steps: 4,\n",
            "  hidden_dropout: 0.3, logging_steps: 0.1, max_epochs: 80, max_grad_norm: 1.0, seed: 42323}\n",
            "verbose: true\n",
            "Loading tokenizer \"wietsedv/bert-base-dutch-cased\"\n",
            " ➤ Loading cached data from cache/getuigenissen-ner-bertje/train.tsv.pkl\n",
            "Train data: 1402 examples, 25 labels: ['B-ACTIVITEIT', 'B-BEDRAG', 'B-BEROEP', 'B-BESCHRIJVING', 'B-CITAAT', 'B-EMOTIE', 'B-GEO', 'B-LEEFTIJD', 'B-MISDRIJF', 'B-OBJECT', 'B-PERSOON', 'B-TIJD', 'I-ACTIVITEIT', 'I-BEDRAG', 'I-BEROEP', 'I-BESCHRIJVING', 'I-CITAAT', 'I-EMOTIE', 'I-GEO', 'I-LEEFTIJD', 'I-MISDRIJF', 'I-OBJECT', 'I-PERSOON', 'I-TIJD', 'O']\n",
            " ➤ Loading cached data from cache/getuigenissen-ner-bertje/dev.tsv.pkl\n",
            "Dev data: 589 examples\n",
            "Loading model \"wietsedv/bert-base-dutch-cased\"\n",
            "Downloading: 100% 439M/439M [00:12<00:00, 35.1MB/s]\n",
            "Some weights of the model checkpoint at wietsedv/bert-base-dutch-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at wietsedv/bert-base-dutch-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Start training\n",
            "Global step intervals: Logging=5 Eval=14\n",
            "Starting at epoch 0\n",
            " > Start epoch 0/80\n",
            "Batch:   8% 18/234 [00:03<00:34,  6.18it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch=0 Step=5 lr=0.000000488 loss=14.019 acc=0.026\n",
            "Epoch=0 Step=10 lr=0.000000977 loss=13.876 acc=0.025\n",
            "Epoch=0 Step=15 lr=0.000001465 loss=13.489 acc=0.031\n",
            "Epoch=0 Step=20 lr=0.000001953 loss=12.943 acc=0.039\n",
            "Epoch=0 Step=25 lr=0.000002441 loss=11.974 acc=0.058\n",
            "Epoch=0 Step=30 lr=0.000002930 loss=10.761 acc=0.095\n",
            "Epoch=0 Step=35 lr=0.000003418 loss=9.473 acc=0.147\n",
            "Epoch=0 Step=40 lr=0.000003906 loss=7.650 acc=0.201\n",
            "Epoch=0 Step=45 lr=0.000004395 loss=5.893 acc=0.262\n",
            "Epoch=0 Step=50 lr=0.000004883 loss=5.044 acc=0.298\n",
            "Epoch=0 Step=55 lr=0.000005371 loss=5.212 acc=0.325\n",
            "Batch: 100% 234/234 [00:42<00:00,  5.48it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "Evaluating:   0% 0/10 [00:00<?, ?it/s]main.py:248: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  mask_idx = label_mask.flatten().nonzero()\n",
            "Evaluating: 100% 10/10 [00:13<00:00,  1.35s/it]\n",
            "Evaluation: Epoch=0 loss=11.894 acc=0.705\n",
            " > Start epoch 1/80\n",
            "Epoch=1 Step=60 lr=0.000005859 loss=5.143 acc=0.683\n",
            "Epoch=1 Step=65 lr=0.000006348 loss=4.976 acc=0.655\n",
            "Epoch=1 Step=70 lr=0.000006836 loss=4.791 acc=0.662\n",
            "Epoch=1 Step=75 lr=0.000007324 loss=4.873 acc=0.658\n",
            "Epoch=1 Step=80 lr=0.000007813 loss=4.303 acc=0.662\n",
            "Epoch=1 Step=85 lr=0.000008301 loss=3.942 acc=0.673\n",
            "Epoch=1 Step=90 lr=0.000008789 loss=3.933 acc=0.676\n",
            "Epoch=1 Step=95 lr=0.000009277 loss=4.110 acc=0.680\n",
            "Epoch=1 Step=100 lr=0.000009766 loss=3.981 acc=0.680\n",
            "Epoch=1 Step=105 lr=0.000010254 loss=3.814 acc=0.681\n",
            "Epoch=1 Step=110 lr=0.000010742 loss=3.859 acc=0.683\n",
            "Epoch=1 Step=115 lr=0.000011230 loss=3.676 acc=0.685\n",
            "Batch: 100% 234/234 [00:44<00:00,  5.25it/s]\n",
            "Evaluating: 100% 10/10 [00:13<00:00,  1.39s/it]\n",
            "Evaluation: Epoch=1 loss=9.518 acc=0.713\n",
            " > Start epoch 2/80\n",
            "Epoch=2 Step=120 lr=0.000011719 loss=4.251 acc=0.698\n",
            "Epoch=2 Step=125 lr=0.000012207 loss=3.776 acc=0.682\n",
            "Epoch=2 Step=130 lr=0.000012695 loss=4.026 acc=0.671\n",
            "Epoch=2 Step=135 lr=0.000013184 loss=3.110 acc=0.690\n",
            "Epoch=2 Step=140 lr=0.000013672 loss=3.414 acc=0.692\n",
            "Epoch=2 Step=145 lr=0.000014160 loss=3.215 acc=0.696\n",
            "Epoch=2 Step=150 lr=0.000014648 loss=3.293 acc=0.698\n",
            "Epoch=2 Step=155 lr=0.000015137 loss=3.017 acc=0.702\n",
            "Epoch=2 Step=160 lr=0.000015625 loss=3.223 acc=0.705\n",
            "Epoch=2 Step=165 lr=0.000016113 loss=2.955 acc=0.706\n",
            "Epoch=2 Step=170 lr=0.000016602 loss=3.398 acc=0.707\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.07it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.43s/it]\n",
            "Evaluation: Epoch=2 loss=7.022 acc=0.768\n",
            " > Start epoch 3/80\n",
            "Epoch=3 Step=175 lr=0.000017090 loss=3.220 acc=0.718\n",
            "Epoch=3 Step=180 lr=0.000017578 loss=3.374 acc=0.716\n",
            "Epoch=3 Step=185 lr=0.000018066 loss=3.322 acc=0.713\n",
            "Epoch=3 Step=190 lr=0.000018555 loss=3.247 acc=0.710\n",
            "Epoch=3 Step=195 lr=0.000019043 loss=2.545 acc=0.721\n",
            "Epoch=3 Step=200 lr=0.000019531 loss=2.365 acc=0.731\n",
            "Epoch=3 Step=205 lr=0.000020020 loss=2.797 acc=0.736\n",
            "Epoch=3 Step=210 lr=0.000020508 loss=2.651 acc=0.741\n",
            "Epoch=3 Step=215 lr=0.000020996 loss=2.807 acc=0.740\n",
            "Epoch=3 Step=220 lr=0.000021484 loss=2.455 acc=0.743\n",
            "Epoch=3 Step=225 lr=0.000021973 loss=2.553 acc=0.744\n",
            "Epoch=3 Step=230 lr=0.000022461 loss=2.640 acc=0.745\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.08it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.44s/it]\n",
            "Evaluation: Epoch=3 loss=6.454 acc=0.771\n",
            " > Start epoch 4/80\n",
            "Epoch=4 Step=235 lr=0.000022949 loss=3.159 acc=0.727\n",
            "Epoch=4 Step=240 lr=0.000023438 loss=2.219 acc=0.769\n",
            "Epoch=4 Step=245 lr=0.000023926 loss=2.801 acc=0.760\n",
            "Epoch=4 Step=250 lr=0.000024414 loss=2.546 acc=0.752\n",
            "Epoch=4 Step=255 lr=0.000024902 loss=2.216 acc=0.762\n",
            "Epoch=4 Step=260 lr=0.000025391 loss=2.370 acc=0.764\n",
            "Epoch=4 Step=265 lr=0.000025879 loss=2.562 acc=0.762\n",
            "Epoch=4 Step=270 lr=0.000026367 loss=2.246 acc=0.763\n",
            "Epoch=4 Step=275 lr=0.000026855 loss=2.328 acc=0.766\n",
            "Epoch=4 Step=280 lr=0.000027344 loss=2.463 acc=0.764\n",
            "Epoch=4 Step=285 lr=0.000027832 loss=2.070 acc=0.767\n",
            "Epoch=4 Step=290 lr=0.000028320 loss=2.293 acc=0.767\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.96it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.45s/it]\n",
            "Evaluation: Epoch=4 loss=5.406 acc=0.790\n",
            " > Start epoch 5/80\n",
            "Epoch=5 Step=295 lr=0.000028809 loss=2.105 acc=0.800\n",
            "Epoch=5 Step=300 lr=0.000029297 loss=2.035 acc=0.798\n",
            "Epoch=5 Step=305 lr=0.000029785 loss=2.208 acc=0.796\n",
            "Epoch=5 Step=310 lr=0.000030273 loss=2.496 acc=0.785\n",
            "Epoch=5 Step=315 lr=0.000030762 loss=1.997 acc=0.788\n",
            "Epoch=5 Step=320 lr=0.000031250 loss=2.066 acc=0.787\n",
            "Epoch=5 Step=325 lr=0.000031738 loss=2.123 acc=0.786\n",
            "Epoch=5 Step=330 lr=0.000032227 loss=1.838 acc=0.789\n",
            "Epoch=5 Step=335 lr=0.000032715 loss=2.286 acc=0.786\n",
            "Epoch=5 Step=340 lr=0.000033203 loss=2.190 acc=0.785\n",
            "Epoch=5 Step=345 lr=0.000033691 loss=1.999 acc=0.786\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.03it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=5 loss=5.331 acc=0.770\n",
            " > Start epoch 6/80\n",
            "Epoch=6 Step=350 lr=0.000034180 loss=1.932 acc=0.850\n",
            "Epoch=6 Step=355 lr=0.000034668 loss=2.078 acc=0.786\n",
            "Epoch=6 Step=360 lr=0.000035156 loss=1.621 acc=0.799\n",
            "Epoch=6 Step=365 lr=0.000035645 loss=1.578 acc=0.803\n",
            "Epoch=6 Step=370 lr=0.000036133 loss=2.002 acc=0.805\n",
            "Epoch=6 Step=375 lr=0.000036621 loss=1.899 acc=0.806\n",
            "Epoch=6 Step=380 lr=0.000037109 loss=1.914 acc=0.802\n",
            "Epoch=6 Step=385 lr=0.000037598 loss=1.899 acc=0.803\n",
            "Epoch=6 Step=390 lr=0.000038086 loss=2.146 acc=0.799\n",
            "Epoch=6 Step=395 lr=0.000038574 loss=1.826 acc=0.801\n",
            "Epoch=6 Step=400 lr=0.000039063 loss=1.712 acc=0.802\n",
            "Epoch=6 Step=405 lr=0.000039551 loss=1.878 acc=0.802\n",
            "Batch: 100% 234/234 [00:46<00:00,  4.99it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=6 loss=5.469 acc=0.752\n",
            " > Start epoch 7/80\n",
            "Epoch=7 Step=410 lr=0.000040039 loss=2.030 acc=0.798\n",
            "Epoch=7 Step=415 lr=0.000040527 loss=1.475 acc=0.828\n",
            "Epoch=7 Step=420 lr=0.000041016 loss=1.741 acc=0.825\n",
            "Epoch=7 Step=425 lr=0.000041504 loss=1.690 acc=0.824\n",
            "Epoch=7 Step=430 lr=0.000041992 loss=1.851 acc=0.820\n",
            "Epoch=7 Step=435 lr=0.000042480 loss=1.740 acc=0.820\n",
            "Epoch=7 Step=440 lr=0.000042969 loss=1.631 acc=0.817\n",
            "Epoch=7 Step=445 lr=0.000043457 loss=1.642 acc=0.817\n",
            "Epoch=7 Step=450 lr=0.000043945 loss=1.655 acc=0.815\n",
            "Epoch=7 Step=455 lr=0.000044434 loss=1.651 acc=0.816\n",
            "Epoch=7 Step=460 lr=0.000044922 loss=1.675 acc=0.814\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.98it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=7 loss=4.896 acc=0.792\n",
            " > Start epoch 8/80\n",
            "Epoch=8 Step=465 lr=0.000045410 loss=1.590 acc=0.801\n",
            "Epoch=8 Step=470 lr=0.000045898 loss=1.613 acc=0.825\n",
            "Epoch=8 Step=475 lr=0.000046387 loss=1.529 acc=0.826\n",
            "Epoch=8 Step=480 lr=0.000046875 loss=1.433 acc=0.833\n",
            "Epoch=8 Step=485 lr=0.000047363 loss=1.541 acc=0.833\n",
            "Epoch=8 Step=490 lr=0.000047852 loss=1.676 acc=0.830\n",
            "Epoch=8 Step=495 lr=0.000048340 loss=1.664 acc=0.825\n",
            "Epoch=8 Step=500 lr=0.000048828 loss=1.470 acc=0.823\n",
            "Epoch=8 Step=505 lr=0.000049316 loss=1.614 acc=0.823\n",
            "Epoch=8 Step=510 lr=0.000049805 loss=1.309 acc=0.828\n",
            "Epoch=8 Step=515 lr=0.000049964 loss=1.449 acc=0.829\n",
            "Epoch=8 Step=520 lr=0.000049903 loss=1.374 acc=0.830\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.95it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=8 loss=5.899 acc=0.731\n",
            " > Start epoch 9/80\n",
            "Epoch=9 Step=525 lr=0.000049843 loss=1.702 acc=0.852\n",
            "Epoch=9 Step=530 lr=0.000049782 loss=1.305 acc=0.844\n",
            "Epoch=9 Step=535 lr=0.000049721 loss=1.229 acc=0.848\n",
            "Epoch=9 Step=540 lr=0.000049661 loss=1.408 acc=0.846\n",
            "Epoch=9 Step=545 lr=0.000049600 loss=1.271 acc=0.847\n",
            "Epoch=9 Step=550 lr=0.000049540 loss=1.556 acc=0.840\n",
            "Epoch=9 Step=555 lr=0.000049479 loss=1.245 acc=0.843\n",
            "Epoch=9 Step=560 lr=0.000049419 loss=1.437 acc=0.844\n",
            "Epoch=9 Step=565 lr=0.000049358 loss=1.337 acc=0.844\n",
            "Epoch=9 Step=570 lr=0.000049297 loss=1.220 acc=0.846\n",
            "Epoch=9 Step=575 lr=0.000049237 loss=1.388 acc=0.846\n",
            "Epoch=9 Step=580 lr=0.000049176 loss=1.136 acc=0.847\n",
            "Batch: 100% 234/234 [00:46<00:00,  4.99it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=9 loss=5.271 acc=0.768\n",
            " > Start epoch 10/80\n",
            "Epoch=10 Step=585 lr=0.000049116 loss=1.354 acc=0.851\n",
            "Epoch=10 Step=590 lr=0.000049055 loss=1.111 acc=0.862\n",
            "Epoch=10 Step=595 lr=0.000048995 loss=0.936 acc=0.870\n",
            "Epoch=10 Step=600 lr=0.000048934 loss=1.159 acc=0.867\n",
            "Epoch=10 Step=605 lr=0.000048874 loss=1.311 acc=0.860\n",
            "Epoch=10 Step=610 lr=0.000048813 loss=1.274 acc=0.859\n",
            "Epoch=10 Step=615 lr=0.000048752 loss=1.166 acc=0.860\n",
            "Epoch=10 Step=620 lr=0.000048692 loss=1.102 acc=0.861\n",
            "Epoch=10 Step=625 lr=0.000048631 loss=1.347 acc=0.857\n",
            "Epoch=10 Step=630 lr=0.000048571 loss=1.271 acc=0.857\n",
            "Epoch=10 Step=635 lr=0.000048510 loss=1.246 acc=0.859\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.85it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=10 loss=5.510 acc=0.757\n",
            " > Start epoch 11/80\n",
            "Epoch=11 Step=640 lr=0.000048450 loss=1.192 acc=0.894\n",
            "Epoch=11 Step=645 lr=0.000048389 loss=1.113 acc=0.876\n",
            "Epoch=11 Step=650 lr=0.000048328 loss=0.996 acc=0.880\n",
            "Epoch=11 Step=655 lr=0.000048268 loss=1.137 acc=0.874\n",
            "Epoch=11 Step=660 lr=0.000048207 loss=1.037 acc=0.872\n",
            "Epoch=11 Step=665 lr=0.000048147 loss=0.938 acc=0.875\n",
            "Epoch=11 Step=670 lr=0.000048086 loss=1.125 acc=0.874\n",
            "Epoch=11 Step=675 lr=0.000048026 loss=1.007 acc=0.875\n",
            "Epoch=11 Step=680 lr=0.000047965 loss=0.997 acc=0.877\n",
            "Epoch=11 Step=685 lr=0.000047905 loss=0.971 acc=0.878\n",
            "Epoch=11 Step=690 lr=0.000047844 loss=1.112 acc=0.877\n",
            "Epoch=11 Step=695 lr=0.000047783 loss=1.005 acc=0.878\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.09it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=11 loss=6.041 acc=0.732\n",
            " > Start epoch 12/80\n",
            "Epoch=12 Step=700 lr=0.000047723 loss=1.279 acc=0.878\n",
            "Epoch=12 Step=705 lr=0.000047662 loss=1.021 acc=0.880\n",
            "Epoch=12 Step=710 lr=0.000047602 loss=1.016 acc=0.879\n",
            "Epoch=12 Step=715 lr=0.000047541 loss=0.906 acc=0.879\n",
            "Epoch=12 Step=720 lr=0.000047481 loss=0.909 acc=0.880\n",
            "Epoch=12 Step=725 lr=0.000047420 loss=0.934 acc=0.879\n",
            "Epoch=12 Step=730 lr=0.000047359 loss=0.948 acc=0.879\n",
            "Epoch=12 Step=735 lr=0.000047299 loss=0.877 acc=0.881\n",
            "Epoch=12 Step=740 lr=0.000047238 loss=0.989 acc=0.880\n",
            "Epoch=12 Step=745 lr=0.000047178 loss=0.938 acc=0.881\n",
            "Epoch=12 Step=750 lr=0.000047117 loss=0.971 acc=0.882\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.95it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=12 loss=6.583 acc=0.718\n",
            " > Start epoch 13/80\n",
            "Epoch=13 Step=755 lr=0.000047057 loss=0.996 acc=0.890\n",
            "Epoch=13 Step=760 lr=0.000046996 loss=0.793 acc=0.893\n",
            "Epoch=13 Step=765 lr=0.000046936 loss=0.851 acc=0.898\n",
            "Epoch=13 Step=770 lr=0.000046875 loss=0.932 acc=0.896\n",
            "Epoch=13 Step=775 lr=0.000046814 loss=0.841 acc=0.897\n",
            "Epoch=13 Step=780 lr=0.000046754 loss=0.899 acc=0.895\n",
            "Epoch=13 Step=785 lr=0.000046693 loss=0.941 acc=0.895\n",
            "Epoch=13 Step=790 lr=0.000046633 loss=1.078 acc=0.892\n",
            "Epoch=13 Step=795 lr=0.000046572 loss=1.013 acc=0.891\n",
            "Epoch=13 Step=800 lr=0.000046512 loss=0.797 acc=0.891\n",
            "Epoch=13 Step=805 lr=0.000046451 loss=0.856 acc=0.891\n",
            "Epoch=13 Step=810 lr=0.000046391 loss=0.673 acc=0.893\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.02it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=13 loss=6.403 acc=0.735\n",
            " > Start epoch 14/80\n",
            "Epoch=14 Step=815 lr=0.000046330 loss=0.962 acc=0.906\n",
            "Epoch=14 Step=820 lr=0.000046269 loss=0.829 acc=0.900\n",
            "Epoch=14 Step=825 lr=0.000046209 loss=0.846 acc=0.898\n",
            "Epoch=14 Step=830 lr=0.000046148 loss=0.770 acc=0.897\n",
            "Epoch=14 Step=835 lr=0.000046088 loss=0.790 acc=0.899\n",
            "Epoch=14 Step=840 lr=0.000046027 loss=0.721 acc=0.901\n",
            "Epoch=14 Step=845 lr=0.000045967 loss=0.777 acc=0.901\n",
            "Epoch=14 Step=850 lr=0.000045906 loss=0.860 acc=0.900\n",
            "Epoch=14 Step=855 lr=0.000045845 loss=0.763 acc=0.901\n",
            "Epoch=14 Step=860 lr=0.000045785 loss=0.799 acc=0.902\n",
            "Epoch=14 Step=865 lr=0.000045724 loss=0.673 acc=0.904\n",
            "Epoch=14 Step=870 lr=0.000045664 loss=0.769 acc=0.903\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.00it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=14 loss=6.474 acc=0.736\n",
            " > Start epoch 15/80\n",
            "Epoch=15 Step=875 lr=0.000045603 loss=0.713 acc=0.917\n",
            "Epoch=15 Step=880 lr=0.000045543 loss=0.733 acc=0.912\n",
            "Epoch=15 Step=885 lr=0.000045482 loss=0.779 acc=0.907\n",
            "Epoch=15 Step=890 lr=0.000045422 loss=0.787 acc=0.909\n",
            "Epoch=15 Step=895 lr=0.000045361 loss=0.809 acc=0.906\n",
            "Epoch=15 Step=900 lr=0.000045300 loss=0.675 acc=0.908\n",
            "Epoch=15 Step=905 lr=0.000045240 loss=0.830 acc=0.907\n",
            "Epoch=15 Step=910 lr=0.000045179 loss=0.626 acc=0.908\n",
            "Epoch=15 Step=915 lr=0.000045119 loss=0.759 acc=0.908\n",
            "Epoch=15 Step=920 lr=0.000045058 loss=0.679 acc=0.909\n",
            "Epoch=15 Step=925 lr=0.000044998 loss=0.800 acc=0.910\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.05it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=15 loss=7.169 acc=0.717\n",
            " > Start epoch 16/80\n",
            "Epoch=16 Step=930 lr=0.000044937 loss=0.769 acc=0.919\n",
            "Epoch=16 Step=935 lr=0.000044876 loss=0.677 acc=0.916\n",
            "Epoch=16 Step=940 lr=0.000044816 loss=0.612 acc=0.919\n",
            "Epoch=16 Step=945 lr=0.000044755 loss=0.665 acc=0.920\n",
            "Epoch=16 Step=950 lr=0.000044695 loss=0.642 acc=0.920\n",
            "Epoch=16 Step=955 lr=0.000044634 loss=0.692 acc=0.918\n",
            "Epoch=16 Step=960 lr=0.000044574 loss=0.761 acc=0.917\n",
            "Epoch=16 Step=965 lr=0.000044513 loss=0.711 acc=0.916\n",
            "Epoch=16 Step=970 lr=0.000044453 loss=0.576 acc=0.916\n",
            "Epoch=16 Step=975 lr=0.000044392 loss=0.585 acc=0.918\n",
            "Epoch=16 Step=980 lr=0.000044331 loss=0.664 acc=0.917\n",
            "Epoch=16 Step=985 lr=0.000044271 loss=0.646 acc=0.917\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.93it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=16 loss=6.586 acc=0.756\n",
            " > Start epoch 17/80\n",
            "Epoch=17 Step=990 lr=0.000044210 loss=0.714 acc=0.911\n",
            "Epoch=17 Step=995 lr=0.000044150 loss=0.741 acc=0.909\n",
            "Epoch=17 Step=1000 lr=0.000044089 loss=0.597 acc=0.915\n",
            "Epoch=17 Step=1005 lr=0.000044029 loss=0.631 acc=0.916\n",
            "Epoch=17 Step=1010 lr=0.000043968 loss=0.556 acc=0.918\n",
            "Epoch=17 Step=1015 lr=0.000043907 loss=0.710 acc=0.917\n",
            "Epoch=17 Step=1020 lr=0.000043847 loss=0.733 acc=0.916\n",
            "Epoch=17 Step=1025 lr=0.000043786 loss=0.585 acc=0.918\n",
            "Epoch=17 Step=1030 lr=0.000043726 loss=0.690 acc=0.918\n",
            "Epoch=17 Step=1035 lr=0.000043665 loss=0.513 acc=0.920\n",
            "Epoch=17 Step=1040 lr=0.000043605 loss=0.629 acc=0.920\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.84it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=17 loss=6.918 acc=0.745\n",
            " > Start epoch 18/80\n",
            "Epoch=18 Step=1045 lr=0.000043544 loss=0.636 acc=0.936\n",
            "Epoch=18 Step=1050 lr=0.000043484 loss=0.592 acc=0.927\n",
            "Epoch=18 Step=1055 lr=0.000043423 loss=0.589 acc=0.926\n",
            "Epoch=18 Step=1060 lr=0.000043362 loss=0.566 acc=0.926\n",
            "Epoch=18 Step=1065 lr=0.000043302 loss=0.512 acc=0.929\n",
            "Epoch=18 Step=1070 lr=0.000043241 loss=0.532 acc=0.929\n",
            "Epoch=18 Step=1075 lr=0.000043181 loss=0.507 acc=0.931\n",
            "Epoch=18 Step=1080 lr=0.000043120 loss=0.615 acc=0.929\n",
            "Epoch=18 Step=1085 lr=0.000043060 loss=0.563 acc=0.928\n",
            "Epoch=18 Step=1090 lr=0.000042999 loss=0.625 acc=0.927\n",
            "Epoch=18 Step=1095 lr=0.000042938 loss=0.541 acc=0.927\n",
            "Epoch=18 Step=1100 lr=0.000042878 loss=0.612 acc=0.928\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.01it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=18 loss=7.310 acc=0.737\n",
            " > Start epoch 19/80\n",
            "Epoch=19 Step=1105 lr=0.000042817 loss=0.629 acc=0.924\n",
            "Epoch=19 Step=1110 lr=0.000042757 loss=0.575 acc=0.928\n",
            "Epoch=19 Step=1115 lr=0.000042696 loss=0.528 acc=0.928\n",
            "Epoch=19 Step=1120 lr=0.000042636 loss=0.475 acc=0.932\n",
            "Epoch=19 Step=1125 lr=0.000042575 loss=0.543 acc=0.932\n",
            "Epoch=19 Step=1130 lr=0.000042515 loss=0.574 acc=0.932\n",
            "Epoch=19 Step=1135 lr=0.000042454 loss=0.432 acc=0.934\n",
            "Epoch=19 Step=1140 lr=0.000042393 loss=0.534 acc=0.934\n",
            "Epoch=19 Step=1145 lr=0.000042333 loss=0.574 acc=0.933\n",
            "Epoch=19 Step=1150 lr=0.000042272 loss=0.591 acc=0.932\n",
            "Epoch=19 Step=1155 lr=0.000042212 loss=0.564 acc=0.932\n",
            "Epoch=19 Step=1160 lr=0.000042151 loss=0.553 acc=0.932\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.91it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=19 loss=6.967 acc=0.762\n",
            " > Start epoch 20/80\n",
            "Epoch=20 Step=1165 lr=0.000042091 loss=0.573 acc=0.933\n",
            "Epoch=20 Step=1170 lr=0.000042030 loss=0.451 acc=0.936\n",
            "Epoch=20 Step=1175 lr=0.000041969 loss=0.390 acc=0.937\n",
            "Epoch=20 Step=1180 lr=0.000041909 loss=0.418 acc=0.940\n",
            "Epoch=20 Step=1185 lr=0.000041848 loss=0.470 acc=0.939\n",
            "Epoch=20 Step=1190 lr=0.000041788 loss=0.390 acc=0.940\n",
            "Epoch=20 Step=1195 lr=0.000041727 loss=0.659 acc=0.938\n",
            "Epoch=20 Step=1200 lr=0.000041667 loss=0.472 acc=0.939\n",
            "Epoch=20 Step=1205 lr=0.000041606 loss=0.526 acc=0.938\n",
            "Epoch=20 Step=1210 lr=0.000041546 loss=0.472 acc=0.938\n",
            "Epoch=20 Step=1215 lr=0.000041485 loss=0.522 acc=0.938\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.97it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=20 loss=7.341 acc=0.734\n",
            " > Start epoch 21/80\n",
            "Epoch=21 Step=1220 lr=0.000041424 loss=0.466 acc=0.966\n",
            "Epoch=21 Step=1225 lr=0.000041364 loss=0.505 acc=0.939\n",
            "Epoch=21 Step=1230 lr=0.000041303 loss=0.469 acc=0.940\n",
            "Epoch=21 Step=1235 lr=0.000041243 loss=0.477 acc=0.940\n",
            "Epoch=21 Step=1240 lr=0.000041182 loss=0.438 acc=0.942\n",
            "Epoch=21 Step=1245 lr=0.000041122 loss=0.413 acc=0.942\n",
            "Epoch=21 Step=1250 lr=0.000041061 loss=0.489 acc=0.940\n",
            "Epoch=21 Step=1255 lr=0.000041000 loss=0.307 acc=0.942\n",
            "Epoch=21 Step=1260 lr=0.000040940 loss=0.487 acc=0.942\n",
            "Epoch=21 Step=1265 lr=0.000040879 loss=0.381 acc=0.943\n",
            "Epoch=21 Step=1270 lr=0.000040819 loss=0.526 acc=0.940\n",
            "Epoch=21 Step=1275 lr=0.000040758 loss=0.382 acc=0.942\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.03it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=21 loss=7.528 acc=0.745\n",
            " > Start epoch 22/80\n",
            "Epoch=22 Step=1280 lr=0.000040698 loss=0.450 acc=0.950\n",
            "Epoch=22 Step=1285 lr=0.000040637 loss=0.391 acc=0.950\n",
            "Epoch=22 Step=1290 lr=0.000040577 loss=0.473 acc=0.947\n",
            "Epoch=22 Step=1295 lr=0.000040516 loss=0.416 acc=0.948\n",
            "Epoch=22 Step=1300 lr=0.000040455 loss=0.328 acc=0.950\n",
            "Epoch=22 Step=1305 lr=0.000040395 loss=0.403 acc=0.950\n",
            "Epoch=22 Step=1310 lr=0.000040334 loss=0.410 acc=0.949\n",
            "Epoch=22 Step=1315 lr=0.000040274 loss=0.516 acc=0.947\n",
            "Epoch=22 Step=1320 lr=0.000040213 loss=0.564 acc=0.944\n",
            "Epoch=22 Step=1325 lr=0.000040153 loss=0.379 acc=0.945\n",
            "Epoch=22 Step=1330 lr=0.000040092 loss=0.344 acc=0.945\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.01it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=22 loss=7.755 acc=0.740\n",
            " > Start epoch 23/80\n",
            "Epoch=23 Step=1335 lr=0.000040031 loss=0.545 acc=0.946\n",
            "Epoch=23 Step=1340 lr=0.000039971 loss=0.533 acc=0.928\n",
            "Epoch=23 Step=1345 lr=0.000039910 loss=0.362 acc=0.939\n",
            "Epoch=23 Step=1350 lr=0.000039850 loss=0.389 acc=0.942\n",
            "Epoch=23 Step=1355 lr=0.000039789 loss=0.473 acc=0.943\n",
            "Epoch=23 Step=1360 lr=0.000039729 loss=0.357 acc=0.945\n",
            "Epoch=23 Step=1365 lr=0.000039668 loss=0.388 acc=0.947\n",
            "Epoch=23 Step=1370 lr=0.000039608 loss=0.330 acc=0.948\n",
            "Epoch=23 Step=1375 lr=0.000039547 loss=0.321 acc=0.949\n",
            "Epoch=23 Step=1380 lr=0.000039486 loss=0.418 acc=0.949\n",
            "Epoch=23 Step=1385 lr=0.000039426 loss=0.373 acc=0.950\n",
            "Epoch=23 Step=1390 lr=0.000039365 loss=0.456 acc=0.949\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.94it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=23 loss=7.539 acc=0.755\n",
            " > Start epoch 24/80\n",
            "Epoch=24 Step=1395 lr=0.000039305 loss=0.520 acc=0.951\n",
            "Epoch=24 Step=1400 lr=0.000039244 loss=0.421 acc=0.942\n",
            "Epoch=24 Step=1405 lr=0.000039184 loss=0.350 acc=0.945\n",
            "Epoch=24 Step=1410 lr=0.000039123 loss=0.448 acc=0.945\n",
            "Epoch=24 Step=1415 lr=0.000039063 loss=0.357 acc=0.947\n",
            "Epoch=24 Step=1420 lr=0.000039002 loss=0.276 acc=0.950\n",
            "Epoch=24 Step=1425 lr=0.000038941 loss=0.422 acc=0.949\n",
            "Epoch=24 Step=1430 lr=0.000038881 loss=0.360 acc=0.950\n",
            "Epoch=24 Step=1435 lr=0.000038820 loss=0.369 acc=0.950\n",
            "Epoch=24 Step=1440 lr=0.000038760 loss=0.407 acc=0.949\n",
            "Epoch=24 Step=1445 lr=0.000038699 loss=0.375 acc=0.950\n",
            "Epoch=24 Step=1450 lr=0.000038639 loss=0.311 acc=0.950\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.00it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=24 loss=7.522 acc=0.759\n",
            " > Start epoch 25/80\n",
            "Epoch=25 Step=1455 lr=0.000038578 loss=0.425 acc=0.950\n",
            "Epoch=25 Step=1460 lr=0.000038517 loss=0.316 acc=0.956\n",
            "Epoch=25 Step=1465 lr=0.000038457 loss=0.374 acc=0.956\n",
            "Epoch=25 Step=1470 lr=0.000038396 loss=0.307 acc=0.957\n",
            "Epoch=25 Step=1475 lr=0.000038336 loss=0.426 acc=0.955\n",
            "Epoch=25 Step=1480 lr=0.000038275 loss=0.344 acc=0.954\n",
            "Epoch=25 Step=1485 lr=0.000038215 loss=0.335 acc=0.955\n",
            "Epoch=25 Step=1490 lr=0.000038154 loss=0.306 acc=0.955\n",
            "Epoch=25 Step=1495 lr=0.000038094 loss=0.408 acc=0.954\n",
            "Epoch=25 Step=1500 lr=0.000038033 loss=0.456 acc=0.953\n",
            "Epoch=25 Step=1505 lr=0.000037972 loss=0.397 acc=0.953\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.04it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=25 loss=8.047 acc=0.743\n",
            " > Start epoch 26/80\n",
            "Epoch=26 Step=1510 lr=0.000037912 loss=0.348 acc=0.954\n",
            "Epoch=26 Step=1515 lr=0.000037851 loss=0.275 acc=0.960\n",
            "Epoch=26 Step=1520 lr=0.000037791 loss=0.327 acc=0.959\n",
            "Epoch=26 Step=1525 lr=0.000037730 loss=0.376 acc=0.957\n",
            "Epoch=26 Step=1530 lr=0.000037670 loss=0.329 acc=0.958\n",
            "Epoch=26 Step=1535 lr=0.000037609 loss=0.304 acc=0.959\n",
            "Epoch=26 Step=1540 lr=0.000037548 loss=0.356 acc=0.958\n",
            "Epoch=26 Step=1545 lr=0.000037488 loss=0.363 acc=0.959\n",
            "Epoch=26 Step=1550 lr=0.000037427 loss=0.377 acc=0.958\n",
            "Epoch=26 Step=1555 lr=0.000037367 loss=0.341 acc=0.958\n",
            "Epoch=26 Step=1560 lr=0.000037306 loss=0.353 acc=0.958\n",
            "Epoch=26 Step=1565 lr=0.000037246 loss=0.304 acc=0.958\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.97it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=26 loss=7.773 acc=0.761\n",
            " > Start epoch 27/80\n",
            "Epoch=27 Step=1570 lr=0.000037185 loss=0.357 acc=0.962\n",
            "Epoch=27 Step=1575 lr=0.000037125 loss=0.349 acc=0.959\n",
            "Epoch=27 Step=1580 lr=0.000037064 loss=0.258 acc=0.962\n",
            "Epoch=27 Step=1585 lr=0.000037003 loss=0.283 acc=0.960\n",
            "Epoch=27 Step=1590 lr=0.000036943 loss=0.355 acc=0.960\n",
            "Epoch=27 Step=1595 lr=0.000036882 loss=0.379 acc=0.958\n",
            "Epoch=27 Step=1600 lr=0.000036822 loss=0.305 acc=0.959\n",
            "Epoch=27 Step=1605 lr=0.000036761 loss=0.328 acc=0.958\n",
            "Epoch=27 Step=1610 lr=0.000036701 loss=0.302 acc=0.959\n",
            "Epoch=27 Step=1615 lr=0.000036640 loss=0.278 acc=0.960\n",
            "Epoch=27 Step=1620 lr=0.000036579 loss=0.332 acc=0.960\n",
            "Batch: 100% 234/234 [00:46<00:00,  4.99it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=27 loss=8.349 acc=0.733\n",
            " > Start epoch 28/80\n",
            "Epoch=28 Step=1625 lr=0.000036519 loss=0.391 acc=0.969\n",
            "Epoch=28 Step=1630 lr=0.000036458 loss=0.358 acc=0.953\n",
            "Epoch=28 Step=1635 lr=0.000036398 loss=0.263 acc=0.960\n",
            "Epoch=28 Step=1640 lr=0.000036337 loss=0.268 acc=0.961\n",
            "Epoch=28 Step=1645 lr=0.000036277 loss=0.277 acc=0.961\n",
            "Epoch=28 Step=1650 lr=0.000036216 loss=0.187 acc=0.964\n",
            "Epoch=28 Step=1655 lr=0.000036156 loss=0.361 acc=0.962\n",
            "Epoch=28 Step=1660 lr=0.000036095 loss=0.417 acc=0.962\n",
            "Epoch=28 Step=1665 lr=0.000036034 loss=0.280 acc=0.962\n",
            "Epoch=28 Step=1670 lr=0.000035974 loss=0.289 acc=0.963\n",
            "Epoch=28 Step=1675 lr=0.000035913 loss=0.306 acc=0.963\n",
            "Epoch=28 Step=1680 lr=0.000035853 loss=0.360 acc=0.962\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.96it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=28 loss=8.079 acc=0.765\n",
            " > Start epoch 29/80\n",
            "Epoch=29 Step=1685 lr=0.000035792 loss=0.447 acc=0.962\n",
            "Epoch=29 Step=1690 lr=0.000035732 loss=0.293 acc=0.965\n",
            "Epoch=29 Step=1695 lr=0.000035671 loss=0.326 acc=0.961\n",
            "Epoch=29 Step=1700 lr=0.000035610 loss=0.263 acc=0.962\n",
            "Epoch=29 Step=1705 lr=0.000035550 loss=0.266 acc=0.963\n",
            "Epoch=29 Step=1710 lr=0.000035489 loss=0.258 acc=0.963\n",
            "Epoch=29 Step=1715 lr=0.000035429 loss=0.262 acc=0.963\n",
            "Epoch=29 Step=1720 lr=0.000035368 loss=0.286 acc=0.963\n",
            "Epoch=29 Step=1725 lr=0.000035308 loss=0.279 acc=0.962\n",
            "Epoch=29 Step=1730 lr=0.000035247 loss=0.256 acc=0.963\n",
            "Epoch=29 Step=1735 lr=0.000035187 loss=0.228 acc=0.964\n",
            "Epoch=29 Step=1740 lr=0.000035126 loss=0.259 acc=0.964\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.96it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=29 loss=8.321 acc=0.758\n",
            " > Start epoch 30/80\n",
            "Epoch=30 Step=1745 lr=0.000035065 loss=0.234 acc=0.969\n",
            "Epoch=30 Step=1750 lr=0.000035005 loss=0.192 acc=0.972\n",
            "Epoch=30 Step=1755 lr=0.000034944 loss=0.198 acc=0.972\n",
            "Epoch=30 Step=1760 lr=0.000034884 loss=0.254 acc=0.970\n",
            "Epoch=30 Step=1765 lr=0.000034823 loss=0.203 acc=0.970\n",
            "Epoch=30 Step=1770 lr=0.000034763 loss=0.258 acc=0.968\n",
            "Epoch=30 Step=1775 lr=0.000034702 loss=0.222 acc=0.968\n",
            "Epoch=30 Step=1780 lr=0.000034641 loss=0.243 acc=0.968\n",
            "Epoch=30 Step=1785 lr=0.000034581 loss=0.294 acc=0.967\n",
            "Epoch=30 Step=1790 lr=0.000034520 loss=0.285 acc=0.967\n",
            "Epoch=30 Step=1795 lr=0.000034460 loss=0.251 acc=0.967\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.04it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=30 loss=9.061 acc=0.742\n",
            " > Start epoch 31/80\n",
            "Epoch=31 Step=1800 lr=0.000034399 loss=0.332 acc=0.967\n",
            "Epoch=31 Step=1805 lr=0.000034339 loss=0.244 acc=0.966\n",
            "Epoch=31 Step=1810 lr=0.000034278 loss=0.259 acc=0.966\n",
            "Epoch=31 Step=1815 lr=0.000034218 loss=0.231 acc=0.967\n",
            "Epoch=31 Step=1820 lr=0.000034157 loss=0.253 acc=0.967\n",
            "Epoch=31 Step=1825 lr=0.000034096 loss=0.268 acc=0.967\n",
            "Epoch=31 Step=1830 lr=0.000034036 loss=0.243 acc=0.967\n",
            "Epoch=31 Step=1835 lr=0.000033975 loss=0.219 acc=0.968\n",
            "Epoch=31 Step=1840 lr=0.000033915 loss=0.260 acc=0.968\n",
            "Epoch=31 Step=1845 lr=0.000033854 loss=0.211 acc=0.968\n",
            "Epoch=31 Step=1850 lr=0.000033794 loss=0.178 acc=0.969\n",
            "Epoch=31 Step=1855 lr=0.000033733 loss=0.268 acc=0.969\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.95it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=31 loss=9.001 acc=0.753\n",
            " > Start epoch 32/80\n",
            "Epoch=32 Step=1860 lr=0.000033672 loss=0.302 acc=0.968\n",
            "Epoch=32 Step=1865 lr=0.000033612 loss=0.221 acc=0.969\n",
            "Epoch=32 Step=1870 lr=0.000033551 loss=0.176 acc=0.972\n",
            "Epoch=32 Step=1875 lr=0.000033491 loss=0.224 acc=0.972\n",
            "Epoch=32 Step=1880 lr=0.000033430 loss=0.194 acc=0.972\n",
            "Epoch=32 Step=1885 lr=0.000033370 loss=0.199 acc=0.972\n",
            "Epoch=32 Step=1890 lr=0.000033309 loss=0.261 acc=0.972\n",
            "Epoch=32 Step=1895 lr=0.000033249 loss=0.205 acc=0.972\n",
            "Epoch=32 Step=1900 lr=0.000033188 loss=0.399 acc=0.971\n",
            "Epoch=32 Step=1905 lr=0.000033127 loss=0.256 acc=0.970\n",
            "Epoch=32 Step=1910 lr=0.000033067 loss=0.220 acc=0.970\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.00it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=32 loss=8.905 acc=0.754\n",
            " > Start epoch 33/80\n",
            "Epoch=33 Step=1915 lr=0.000033006 loss=0.260 acc=0.985\n",
            "Epoch=33 Step=1920 lr=0.000032946 loss=0.256 acc=0.969\n",
            "Epoch=33 Step=1925 lr=0.000032885 loss=0.201 acc=0.972\n",
            "Epoch=33 Step=1930 lr=0.000032825 loss=0.194 acc=0.974\n",
            "Epoch=33 Step=1935 lr=0.000032764 loss=0.183 acc=0.975\n",
            "Epoch=33 Step=1940 lr=0.000032703 loss=0.181 acc=0.975\n",
            "Epoch=33 Step=1945 lr=0.000032643 loss=0.170 acc=0.975\n",
            "Epoch=33 Step=1950 lr=0.000032582 loss=0.334 acc=0.973\n",
            "Epoch=33 Step=1955 lr=0.000032522 loss=0.289 acc=0.972\n",
            "Epoch=33 Step=1960 lr=0.000032461 loss=0.234 acc=0.972\n",
            "Epoch=33 Step=1965 lr=0.000032401 loss=0.146 acc=0.973\n",
            "Epoch=33 Step=1970 lr=0.000032340 loss=0.207 acc=0.973\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.06it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=33 loss=9.138 acc=0.760\n",
            " > Start epoch 34/80\n",
            "Epoch=34 Step=1975 lr=0.000032280 loss=0.207 acc=0.981\n",
            "Epoch=34 Step=1980 lr=0.000032219 loss=0.182 acc=0.979\n",
            "Epoch=34 Step=1985 lr=0.000032158 loss=0.194 acc=0.978\n",
            "Epoch=34 Step=1990 lr=0.000032098 loss=0.214 acc=0.977\n",
            "Epoch=34 Step=1995 lr=0.000032037 loss=0.234 acc=0.975\n",
            "Epoch=34 Step=2000 lr=0.000031977 loss=0.184 acc=0.975\n",
            "Epoch=34 Step=2005 lr=0.000031916 loss=0.222 acc=0.975\n",
            "Epoch=34 Step=2010 lr=0.000031856 loss=0.328 acc=0.973\n",
            "Epoch=34 Step=2015 lr=0.000031795 loss=0.229 acc=0.973\n",
            "Epoch=34 Step=2020 lr=0.000031734 loss=0.208 acc=0.973\n",
            "Epoch=34 Step=2025 lr=0.000031674 loss=0.223 acc=0.972\n",
            "Epoch=34 Step=2030 lr=0.000031613 loss=0.237 acc=0.972\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.96it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=34 loss=8.969 acc=0.766\n",
            " > Start epoch 35/80\n",
            "Epoch=35 Step=2035 lr=0.000031553 loss=0.255 acc=0.967\n",
            "Epoch=35 Step=2040 lr=0.000031492 loss=0.190 acc=0.971\n",
            "Epoch=35 Step=2045 lr=0.000031432 loss=0.186 acc=0.973\n",
            "Epoch=35 Step=2050 lr=0.000031371 loss=0.141 acc=0.975\n",
            "Epoch=35 Step=2055 lr=0.000031311 loss=0.220 acc=0.974\n",
            "Epoch=35 Step=2060 lr=0.000031250 loss=0.181 acc=0.974\n",
            "Epoch=35 Step=2065 lr=0.000031189 loss=0.241 acc=0.974\n",
            "Epoch=35 Step=2070 lr=0.000031129 loss=0.171 acc=0.974\n",
            "Epoch=35 Step=2075 lr=0.000031068 loss=0.281 acc=0.974\n",
            "Epoch=35 Step=2080 lr=0.000031008 loss=0.187 acc=0.974\n",
            "Epoch=35 Step=2085 lr=0.000030947 loss=0.223 acc=0.974\n",
            "Batch: 100% 234/234 [00:46<00:00,  4.98it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=35 loss=9.333 acc=0.750\n",
            " > Start epoch 36/80\n",
            "Epoch=36 Step=2090 lr=0.000030887 loss=0.206 acc=0.970\n",
            "Epoch=36 Step=2095 lr=0.000030826 loss=0.168 acc=0.977\n",
            "Epoch=36 Step=2100 lr=0.000030766 loss=0.149 acc=0.979\n",
            "Epoch=36 Step=2105 lr=0.000030705 loss=0.148 acc=0.980\n",
            "Epoch=36 Step=2110 lr=0.000030644 loss=0.199 acc=0.978\n",
            "Epoch=36 Step=2115 lr=0.000030584 loss=0.185 acc=0.978\n",
            "Epoch=36 Step=2120 lr=0.000030523 loss=0.164 acc=0.978\n",
            "Epoch=36 Step=2125 lr=0.000030463 loss=0.170 acc=0.978\n",
            "Epoch=36 Step=2130 lr=0.000030402 loss=0.205 acc=0.978\n",
            "Epoch=36 Step=2135 lr=0.000030342 loss=0.207 acc=0.977\n",
            "Epoch=36 Step=2140 lr=0.000030281 loss=0.162 acc=0.977\n",
            "Epoch=36 Step=2145 lr=0.000030220 loss=0.151 acc=0.977\n",
            "Batch: 100% 234/234 [00:45<00:00,  5.09it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=36 loss=9.604 acc=0.751\n",
            " > Start epoch 37/80\n",
            "Epoch=37 Step=2150 lr=0.000030160 loss=0.152 acc=0.986\n",
            "Epoch=37 Step=2155 lr=0.000030099 loss=0.198 acc=0.979\n",
            "Epoch=37 Step=2160 lr=0.000030039 loss=0.198 acc=0.977\n",
            "Epoch=37 Step=2165 lr=0.000029978 loss=0.220 acc=0.976\n",
            "Epoch=37 Step=2170 lr=0.000029918 loss=0.196 acc=0.977\n",
            "Epoch=37 Step=2175 lr=0.000029857 loss=0.228 acc=0.976\n",
            "Epoch=37 Step=2180 lr=0.000029797 loss=0.160 acc=0.976\n",
            "Epoch=37 Step=2185 lr=0.000029736 loss=0.199 acc=0.976\n",
            "Epoch=37 Step=2190 lr=0.000029675 loss=0.150 acc=0.976\n",
            "Epoch=37 Step=2195 lr=0.000029615 loss=0.130 acc=0.977\n",
            "Epoch=37 Step=2200 lr=0.000029554 loss=0.203 acc=0.977\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.01it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=37 loss=9.630 acc=0.748\n",
            " > Start epoch 38/80\n",
            "Epoch=38 Step=2205 lr=0.000029494 loss=0.232 acc=0.993\n",
            "Epoch=38 Step=2210 lr=0.000029433 loss=0.133 acc=0.983\n",
            "Epoch=38 Step=2215 lr=0.000029373 loss=0.153 acc=0.982\n",
            "Epoch=38 Step=2220 lr=0.000029312 loss=0.223 acc=0.979\n",
            "Epoch=38 Step=2225 lr=0.000029251 loss=0.164 acc=0.978\n",
            "Epoch=38 Step=2230 lr=0.000029191 loss=0.170 acc=0.977\n",
            "Epoch=38 Step=2235 lr=0.000029130 loss=0.136 acc=0.978\n",
            "Epoch=38 Step=2240 lr=0.000029070 loss=0.273 acc=0.976\n",
            "Epoch=38 Step=2245 lr=0.000029009 loss=0.152 acc=0.976\n",
            "Epoch=38 Step=2250 lr=0.000028949 loss=0.171 acc=0.976\n",
            "Epoch=38 Step=2255 lr=0.000028888 loss=0.156 acc=0.976\n",
            "Epoch=38 Step=2260 lr=0.000028828 loss=0.200 acc=0.976\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.01it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=38 loss=9.663 acc=0.756\n",
            " > Start epoch 39/80\n",
            "Epoch=39 Step=2265 lr=0.000028767 loss=0.115 acc=0.983\n",
            "Epoch=39 Step=2270 lr=0.000028706 loss=0.182 acc=0.977\n",
            "Epoch=39 Step=2275 lr=0.000028646 loss=0.147 acc=0.978\n",
            "Epoch=39 Step=2280 lr=0.000028585 loss=0.138 acc=0.979\n",
            "Epoch=39 Step=2285 lr=0.000028525 loss=0.167 acc=0.979\n",
            "Epoch=39 Step=2290 lr=0.000028464 loss=0.205 acc=0.978\n",
            "Epoch=39 Step=2295 lr=0.000028404 loss=0.152 acc=0.979\n",
            "Epoch=39 Step=2300 lr=0.000028343 loss=0.184 acc=0.978\n",
            "Epoch=39 Step=2305 lr=0.000028282 loss=0.197 acc=0.978\n",
            "Epoch=39 Step=2310 lr=0.000028222 loss=0.177 acc=0.978\n",
            "Epoch=39 Step=2315 lr=0.000028161 loss=0.138 acc=0.978\n",
            "Epoch=39 Step=2320 lr=0.000028101 loss=0.169 acc=0.978\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.97it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=39 loss=9.569 acc=0.758\n",
            " > Start epoch 40/80\n",
            "Epoch=40 Step=2325 lr=0.000028040 loss=0.163 acc=0.980\n",
            "Epoch=40 Step=2330 lr=0.000027980 loss=0.165 acc=0.979\n",
            "Epoch=40 Step=2335 lr=0.000027919 loss=0.127 acc=0.980\n",
            "Epoch=40 Step=2340 lr=0.000027859 loss=0.141 acc=0.980\n",
            "Epoch=40 Step=2345 lr=0.000027798 loss=0.172 acc=0.980\n",
            "Epoch=40 Step=2350 lr=0.000027737 loss=0.140 acc=0.981\n",
            "Epoch=40 Step=2355 lr=0.000027677 loss=0.148 acc=0.981\n",
            "Epoch=40 Step=2360 lr=0.000027616 loss=0.167 acc=0.981\n",
            "Epoch=40 Step=2365 lr=0.000027556 loss=0.163 acc=0.981\n",
            "Epoch=40 Step=2370 lr=0.000027495 loss=0.128 acc=0.981\n",
            "Epoch=40 Step=2375 lr=0.000027435 loss=0.188 acc=0.980\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.97it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=40 loss=9.881 acc=0.751\n",
            " > Start epoch 41/80\n",
            "Epoch=41 Step=2380 lr=0.000027374 loss=0.179 acc=0.984\n",
            "Epoch=41 Step=2385 lr=0.000027313 loss=0.133 acc=0.983\n",
            "Epoch=41 Step=2390 lr=0.000027253 loss=0.116 acc=0.983\n",
            "Epoch=41 Step=2395 lr=0.000027192 loss=0.131 acc=0.983\n",
            "Epoch=41 Step=2400 lr=0.000027132 loss=0.203 acc=0.983\n",
            "Epoch=41 Step=2405 lr=0.000027071 loss=0.105 acc=0.983\n",
            "Epoch=41 Step=2410 lr=0.000027011 loss=0.140 acc=0.983\n",
            "Epoch=41 Step=2415 lr=0.000026950 loss=0.154 acc=0.982\n",
            "Epoch=41 Step=2420 lr=0.000026890 loss=0.151 acc=0.982\n",
            "Epoch=41 Step=2425 lr=0.000026829 loss=0.120 acc=0.982\n",
            "Epoch=41 Step=2430 lr=0.000026768 loss=0.208 acc=0.981\n",
            "Epoch=41 Step=2435 lr=0.000026708 loss=0.140 acc=0.981\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.03it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=41 loss=9.617 acc=0.767\n",
            " > Start epoch 42/80\n",
            "Epoch=42 Step=2440 lr=0.000026647 loss=0.207 acc=0.977\n",
            "Epoch=42 Step=2445 lr=0.000026587 loss=0.087 acc=0.982\n",
            "Epoch=42 Step=2450 lr=0.000026526 loss=0.131 acc=0.983\n",
            "Epoch=42 Step=2455 lr=0.000026466 loss=0.147 acc=0.982\n",
            "Epoch=42 Step=2460 lr=0.000026405 loss=0.127 acc=0.982\n",
            "Epoch=42 Step=2465 lr=0.000026344 loss=0.153 acc=0.981\n",
            "Epoch=42 Step=2470 lr=0.000026284 loss=0.157 acc=0.981\n",
            "Epoch=42 Step=2475 lr=0.000026223 loss=0.179 acc=0.981\n",
            "Epoch=42 Step=2480 lr=0.000026163 loss=0.162 acc=0.981\n",
            "Epoch=42 Step=2485 lr=0.000026102 loss=0.118 acc=0.981\n",
            "Epoch=42 Step=2490 lr=0.000026042 loss=0.130 acc=0.981\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.04it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=42 loss=9.999 acc=0.757\n",
            " > Start epoch 43/80\n",
            "Epoch=43 Step=2495 lr=0.000025981 loss=0.170 acc=0.985\n",
            "Epoch=43 Step=2500 lr=0.000025921 loss=0.100 acc=0.987\n",
            "Epoch=43 Step=2505 lr=0.000025860 loss=0.109 acc=0.986\n",
            "Epoch=43 Step=2510 lr=0.000025799 loss=0.174 acc=0.984\n",
            "Epoch=43 Step=2515 lr=0.000025739 loss=0.164 acc=0.983\n",
            "Epoch=43 Step=2520 lr=0.000025678 loss=0.177 acc=0.982\n",
            "Epoch=43 Step=2525 lr=0.000025618 loss=0.126 acc=0.982\n",
            "Epoch=43 Step=2530 lr=0.000025557 loss=0.157 acc=0.982\n",
            "Epoch=43 Step=2535 lr=0.000025497 loss=0.156 acc=0.981\n",
            "Epoch=43 Step=2540 lr=0.000025436 loss=0.139 acc=0.981\n",
            "Epoch=43 Step=2545 lr=0.000025375 loss=0.127 acc=0.981\n",
            "Epoch=43 Step=2550 lr=0.000025315 loss=0.136 acc=0.981\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.01it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=43 loss=10.010 acc=0.756\n",
            " > Start epoch 44/80\n",
            "Epoch=44 Step=2555 lr=0.000025254 loss=0.134 acc=0.985\n",
            "Epoch=44 Step=2560 lr=0.000025194 loss=0.136 acc=0.982\n",
            "Epoch=44 Step=2565 lr=0.000025133 loss=0.112 acc=0.982\n",
            "Epoch=44 Step=2570 lr=0.000025073 loss=0.125 acc=0.983\n",
            "Epoch=44 Step=2575 lr=0.000025012 loss=0.102 acc=0.983\n",
            "Epoch=44 Step=2580 lr=0.000024952 loss=0.148 acc=0.983\n",
            "Epoch=44 Step=2585 lr=0.000024891 loss=0.132 acc=0.982\n",
            "Epoch=44 Step=2590 lr=0.000024830 loss=0.164 acc=0.983\n",
            "Epoch=44 Step=2595 lr=0.000024770 loss=0.136 acc=0.983\n",
            "Epoch=44 Step=2600 lr=0.000024709 loss=0.104 acc=0.983\n",
            "Epoch=44 Step=2605 lr=0.000024649 loss=0.140 acc=0.983\n",
            "Epoch=44 Step=2610 lr=0.000024588 loss=0.165 acc=0.983\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.00it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.46s/it]\n",
            "Evaluation: Epoch=44 loss=10.069 acc=0.759\n",
            " > Start epoch 45/80\n",
            "Epoch=45 Step=2615 lr=0.000024528 loss=0.146 acc=0.981\n",
            "Epoch=45 Step=2620 lr=0.000024467 loss=0.102 acc=0.984\n",
            "Epoch=45 Step=2625 lr=0.000024406 loss=0.136 acc=0.983\n",
            "Epoch=45 Step=2630 lr=0.000024346 loss=0.146 acc=0.983\n",
            "Epoch=45 Step=2635 lr=0.000024285 loss=0.109 acc=0.984\n",
            "Epoch=45 Step=2640 lr=0.000024225 loss=0.120 acc=0.983\n",
            "Epoch=45 Step=2645 lr=0.000024164 loss=0.082 acc=0.984\n",
            "Epoch=45 Step=2650 lr=0.000024104 loss=0.142 acc=0.984\n",
            "Epoch=45 Step=2655 lr=0.000024043 loss=0.138 acc=0.983\n",
            "Epoch=45 Step=2660 lr=0.000023983 loss=0.156 acc=0.983\n",
            "Epoch=45 Step=2665 lr=0.000023922 loss=0.154 acc=0.983\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.00it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=45 loss=9.894 acc=0.766\n",
            " > Start epoch 46/80\n",
            "Epoch=46 Step=2670 lr=0.000023861 loss=0.098 acc=0.986\n",
            "Epoch=46 Step=2675 lr=0.000023801 loss=0.115 acc=0.985\n",
            "Epoch=46 Step=2680 lr=0.000023740 loss=0.113 acc=0.985\n",
            "Epoch=46 Step=2685 lr=0.000023680 loss=0.129 acc=0.985\n",
            "Epoch=46 Step=2690 lr=0.000023619 loss=0.155 acc=0.983\n",
            "Epoch=46 Step=2695 lr=0.000023559 loss=0.129 acc=0.983\n",
            "Epoch=46 Step=2700 lr=0.000023498 loss=0.139 acc=0.983\n",
            "Epoch=46 Step=2705 lr=0.000023438 loss=0.082 acc=0.984\n",
            "Epoch=46 Step=2710 lr=0.000023377 loss=0.104 acc=0.984\n",
            "Epoch=46 Step=2715 lr=0.000023316 loss=0.095 acc=0.985\n",
            "Epoch=46 Step=2720 lr=0.000023256 loss=0.200 acc=0.983\n",
            "Epoch=46 Step=2725 lr=0.000023195 loss=0.125 acc=0.983\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.83it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=46 loss=10.065 acc=0.760\n",
            " > Start epoch 47/80\n",
            "Epoch=47 Step=2730 lr=0.000023135 loss=0.128 acc=0.988\n",
            "Epoch=47 Step=2735 lr=0.000023074 loss=0.120 acc=0.986\n",
            "Epoch=47 Step=2740 lr=0.000023014 loss=0.100 acc=0.986\n",
            "Epoch=47 Step=2745 lr=0.000022953 loss=0.092 acc=0.987\n",
            "Epoch=47 Step=2750 lr=0.000022892 loss=0.096 acc=0.987\n",
            "Epoch=47 Step=2755 lr=0.000022832 loss=0.135 acc=0.986\n",
            "Epoch=47 Step=2760 lr=0.000022771 loss=0.116 acc=0.986\n",
            "Epoch=47 Step=2765 lr=0.000022711 loss=0.093 acc=0.986\n",
            "Epoch=47 Step=2770 lr=0.000022650 loss=0.105 acc=0.986\n",
            "Epoch=47 Step=2775 lr=0.000022590 loss=0.143 acc=0.985\n",
            "Epoch=47 Step=2780 lr=0.000022529 loss=0.136 acc=0.985\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.86it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=47 loss=10.146 acc=0.767\n",
            " > Start epoch 48/80\n",
            "Epoch=48 Step=2785 lr=0.000022469 loss=0.117 acc=0.994\n",
            "Epoch=48 Step=2790 lr=0.000022408 loss=0.140 acc=0.985\n",
            "Epoch=48 Step=2795 lr=0.000022347 loss=0.095 acc=0.987\n",
            "Epoch=48 Step=2800 lr=0.000022287 loss=0.152 acc=0.984\n",
            "Epoch=48 Step=2805 lr=0.000022226 loss=0.084 acc=0.986\n",
            "Epoch=48 Step=2810 lr=0.000022166 loss=0.100 acc=0.986\n",
            "Epoch=48 Step=2815 lr=0.000022105 loss=0.109 acc=0.987\n",
            "Epoch=48 Step=2820 lr=0.000022045 loss=0.116 acc=0.986\n",
            "Epoch=48 Step=2825 lr=0.000021984 loss=0.151 acc=0.985\n",
            "Epoch=48 Step=2830 lr=0.000021923 loss=0.123 acc=0.985\n",
            "Epoch=48 Step=2835 lr=0.000021863 loss=0.111 acc=0.985\n",
            "Epoch=48 Step=2840 lr=0.000021802 loss=0.093 acc=0.985\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.94it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=48 loss=10.254 acc=0.765\n",
            " > Start epoch 49/80\n",
            "Epoch=49 Step=2845 lr=0.000021742 loss=0.117 acc=0.989\n",
            "Epoch=49 Step=2850 lr=0.000021681 loss=0.111 acc=0.986\n",
            "Epoch=49 Step=2855 lr=0.000021621 loss=0.088 acc=0.987\n",
            "Epoch=49 Step=2860 lr=0.000021560 loss=0.112 acc=0.986\n",
            "Epoch=49 Step=2865 lr=0.000021500 loss=0.101 acc=0.986\n",
            "Epoch=49 Step=2870 lr=0.000021439 loss=0.085 acc=0.986\n",
            "Epoch=49 Step=2875 lr=0.000021378 loss=0.115 acc=0.986\n",
            "Epoch=49 Step=2880 lr=0.000021318 loss=0.147 acc=0.985\n",
            "Epoch=49 Step=2885 lr=0.000021257 loss=0.085 acc=0.986\n",
            "Epoch=49 Step=2890 lr=0.000021197 loss=0.084 acc=0.986\n",
            "Epoch=49 Step=2895 lr=0.000021136 loss=0.123 acc=0.986\n",
            "Epoch=49 Step=2900 lr=0.000021076 loss=0.107 acc=0.986\n",
            "Batch: 100% 234/234 [00:46<00:00,  4.99it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=49 loss=10.721 acc=0.758\n",
            " > Start epoch 50/80\n",
            "Epoch=50 Step=2905 lr=0.000021015 loss=0.187 acc=0.986\n",
            "Epoch=50 Step=2910 lr=0.000020954 loss=0.126 acc=0.983\n",
            "Epoch=50 Step=2915 lr=0.000020894 loss=0.102 acc=0.984\n",
            "Epoch=50 Step=2920 lr=0.000020833 loss=0.128 acc=0.984\n",
            "Epoch=50 Step=2925 lr=0.000020773 loss=0.135 acc=0.983\n",
            "Epoch=50 Step=2930 lr=0.000020712 loss=0.106 acc=0.984\n",
            "Epoch=50 Step=2935 lr=0.000020652 loss=0.121 acc=0.984\n",
            "Epoch=50 Step=2940 lr=0.000020591 loss=0.108 acc=0.984\n",
            "Epoch=50 Step=2945 lr=0.000020531 loss=0.096 acc=0.984\n",
            "Epoch=50 Step=2950 lr=0.000020470 loss=0.098 acc=0.985\n",
            "Epoch=50 Step=2955 lr=0.000020409 loss=0.106 acc=0.985\n",
            "Batch: 100% 234/234 [00:46<00:00,  4.98it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=50 loss=10.338 acc=0.766\n",
            " > Start epoch 51/80\n",
            "Epoch=51 Step=2960 lr=0.000020349 loss=0.098 acc=0.995\n",
            "Epoch=51 Step=2965 lr=0.000020288 loss=0.097 acc=0.989\n",
            "Epoch=51 Step=2970 lr=0.000020228 loss=0.091 acc=0.989\n",
            "Epoch=51 Step=2975 lr=0.000020167 loss=0.112 acc=0.988\n",
            "Epoch=51 Step=2980 lr=0.000020107 loss=0.090 acc=0.988\n",
            "Epoch=51 Step=2985 lr=0.000020046 loss=0.079 acc=0.988\n",
            "Epoch=51 Step=2990 lr=0.000019985 loss=0.073 acc=0.988\n",
            "Epoch=51 Step=2995 lr=0.000019925 loss=0.092 acc=0.988\n",
            "Epoch=51 Step=3000 lr=0.000019864 loss=0.110 acc=0.988\n",
            "Epoch=51 Step=3005 lr=0.000019804 loss=0.127 acc=0.987\n",
            "Epoch=51 Step=3010 lr=0.000019743 loss=0.088 acc=0.987\n",
            "Epoch=51 Step=3015 lr=0.000019683 loss=0.121 acc=0.987\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.94it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=51 loss=11.027 acc=0.750\n",
            " > Start epoch 52/80\n",
            "Epoch=52 Step=3020 lr=0.000019622 loss=0.102 acc=0.983\n",
            "Epoch=52 Step=3025 lr=0.000019562 loss=0.089 acc=0.987\n",
            "Epoch=52 Step=3030 lr=0.000019501 loss=0.105 acc=0.986\n",
            "Epoch=52 Step=3035 lr=0.000019440 loss=0.076 acc=0.987\n",
            "Epoch=52 Step=3040 lr=0.000019380 loss=0.083 acc=0.987\n",
            "Epoch=52 Step=3045 lr=0.000019319 loss=0.110 acc=0.987\n",
            "Epoch=52 Step=3050 lr=0.000019259 loss=0.068 acc=0.987\n",
            "Epoch=52 Step=3055 lr=0.000019198 loss=0.099 acc=0.987\n",
            "Epoch=52 Step=3060 lr=0.000019138 loss=0.108 acc=0.988\n",
            "Epoch=52 Step=3065 lr=0.000019077 loss=0.070 acc=0.988\n",
            "Epoch=52 Step=3070 lr=0.000019016 loss=0.097 acc=0.988\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.96it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=52 loss=10.680 acc=0.764\n",
            " > Start epoch 53/80\n",
            "Epoch=53 Step=3075 lr=0.000018956 loss=0.116 acc=0.993\n",
            "Epoch=53 Step=3080 lr=0.000018895 loss=0.086 acc=0.989\n",
            "Epoch=53 Step=3085 lr=0.000018835 loss=0.124 acc=0.987\n",
            "Epoch=53 Step=3090 lr=0.000018774 loss=0.089 acc=0.988\n",
            "Epoch=53 Step=3095 lr=0.000018714 loss=0.116 acc=0.987\n",
            "Epoch=53 Step=3100 lr=0.000018653 loss=0.080 acc=0.988\n",
            "Epoch=53 Step=3105 lr=0.000018593 loss=0.127 acc=0.987\n",
            "Epoch=53 Step=3110 lr=0.000018532 loss=0.096 acc=0.987\n",
            "Epoch=53 Step=3115 lr=0.000018471 loss=0.093 acc=0.987\n",
            "Epoch=53 Step=3120 lr=0.000018411 loss=0.098 acc=0.987\n",
            "Epoch=53 Step=3125 lr=0.000018350 loss=0.102 acc=0.987\n",
            "Epoch=53 Step=3130 lr=0.000018290 loss=0.116 acc=0.987\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.82it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=53 loss=10.559 acc=0.762\n",
            " > Start epoch 54/80\n",
            "Epoch=54 Step=3135 lr=0.000018229 loss=0.098 acc=0.987\n",
            "Epoch=54 Step=3140 lr=0.000018169 loss=0.068 acc=0.991\n",
            "Epoch=54 Step=3145 lr=0.000018108 loss=0.101 acc=0.990\n",
            "Epoch=54 Step=3150 lr=0.000018047 loss=0.137 acc=0.989\n",
            "Epoch=54 Step=3155 lr=0.000017987 loss=0.088 acc=0.989\n",
            "Epoch=54 Step=3160 lr=0.000017926 loss=0.086 acc=0.989\n",
            "Epoch=54 Step=3165 lr=0.000017866 loss=0.072 acc=0.990\n",
            "Epoch=54 Step=3170 lr=0.000017805 loss=0.076 acc=0.990\n",
            "Epoch=54 Step=3175 lr=0.000017745 loss=0.069 acc=0.990\n",
            "Epoch=54 Step=3180 lr=0.000017684 loss=0.136 acc=0.989\n",
            "Epoch=54 Step=3185 lr=0.000017624 loss=0.136 acc=0.989\n",
            "Epoch=54 Step=3190 lr=0.000017563 loss=0.084 acc=0.989\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.85it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=54 loss=11.248 acc=0.747\n",
            " > Start epoch 55/80\n",
            "Epoch=55 Step=3195 lr=0.000017502 loss=0.099 acc=0.988\n",
            "Epoch=55 Step=3200 lr=0.000017442 loss=0.114 acc=0.987\n",
            "Epoch=55 Step=3205 lr=0.000017381 loss=0.078 acc=0.988\n",
            "Epoch=55 Step=3210 lr=0.000017321 loss=0.144 acc=0.987\n",
            "Epoch=55 Step=3215 lr=0.000017260 loss=0.075 acc=0.987\n",
            "Epoch=55 Step=3220 lr=0.000017200 loss=0.084 acc=0.988\n",
            "Epoch=55 Step=3225 lr=0.000017139 loss=0.105 acc=0.987\n",
            "Epoch=55 Step=3230 lr=0.000017078 loss=0.079 acc=0.987\n",
            "Epoch=55 Step=3235 lr=0.000017018 loss=0.056 acc=0.988\n",
            "Epoch=55 Step=3240 lr=0.000016957 loss=0.127 acc=0.988\n",
            "Epoch=55 Step=3245 lr=0.000016897 loss=0.077 acc=0.988\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.93it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=55 loss=10.978 acc=0.756\n",
            " > Start epoch 56/80\n",
            "Epoch=56 Step=3250 lr=0.000016836 loss=0.076 acc=0.991\n",
            "Epoch=56 Step=3255 lr=0.000016776 loss=0.075 acc=0.991\n",
            "Epoch=56 Step=3260 lr=0.000016715 loss=0.115 acc=0.989\n",
            "Epoch=56 Step=3265 lr=0.000016655 loss=0.068 acc=0.989\n",
            "Epoch=56 Step=3270 lr=0.000016594 loss=0.114 acc=0.988\n",
            "Epoch=56 Step=3275 lr=0.000016533 loss=0.082 acc=0.988\n",
            "Epoch=56 Step=3280 lr=0.000016473 loss=0.101 acc=0.989\n",
            "Epoch=56 Step=3285 lr=0.000016412 loss=0.060 acc=0.989\n",
            "Epoch=56 Step=3290 lr=0.000016352 loss=0.063 acc=0.989\n",
            "Epoch=56 Step=3295 lr=0.000016291 loss=0.109 acc=0.989\n",
            "Epoch=56 Step=3300 lr=0.000016231 loss=0.089 acc=0.989\n",
            "Epoch=56 Step=3305 lr=0.000016170 loss=0.069 acc=0.989\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.94it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=56 loss=10.676 acc=0.768\n",
            " > Start epoch 57/80\n",
            "Epoch=57 Step=3310 lr=0.000016109 loss=0.083 acc=0.991\n",
            "Epoch=57 Step=3315 lr=0.000016049 loss=0.125 acc=0.988\n",
            "Epoch=57 Step=3320 lr=0.000015988 loss=0.091 acc=0.987\n",
            "Epoch=57 Step=3325 lr=0.000015928 loss=0.061 acc=0.989\n",
            "Epoch=57 Step=3330 lr=0.000015867 loss=0.066 acc=0.989\n",
            "Epoch=57 Step=3335 lr=0.000015807 loss=0.085 acc=0.989\n",
            "Epoch=57 Step=3340 lr=0.000015746 loss=0.064 acc=0.989\n",
            "Epoch=57 Step=3345 lr=0.000015686 loss=0.079 acc=0.989\n",
            "Epoch=57 Step=3350 lr=0.000015625 loss=0.095 acc=0.989\n",
            "Epoch=57 Step=3355 lr=0.000015564 loss=0.075 acc=0.989\n",
            "Epoch=57 Step=3360 lr=0.000015504 loss=0.086 acc=0.989\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.87it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=57 loss=11.016 acc=0.761\n",
            " > Start epoch 58/80\n",
            "Epoch=58 Step=3365 lr=0.000015443 loss=0.101 acc=0.970\n",
            "Epoch=58 Step=3370 lr=0.000015383 loss=0.053 acc=0.990\n",
            "Epoch=58 Step=3375 lr=0.000015322 loss=0.079 acc=0.990\n",
            "Epoch=58 Step=3380 lr=0.000015262 loss=0.080 acc=0.989\n",
            "Epoch=58 Step=3385 lr=0.000015201 loss=0.062 acc=0.990\n",
            "Epoch=58 Step=3390 lr=0.000015141 loss=0.063 acc=0.990\n",
            "Epoch=58 Step=3395 lr=0.000015080 loss=0.096 acc=0.990\n",
            "Epoch=58 Step=3400 lr=0.000015019 loss=0.059 acc=0.990\n",
            "Epoch=58 Step=3405 lr=0.000014959 loss=0.121 acc=0.990\n",
            "Epoch=58 Step=3410 lr=0.000014898 loss=0.074 acc=0.990\n",
            "Epoch=58 Step=3415 lr=0.000014838 loss=0.072 acc=0.990\n",
            "Epoch=58 Step=3420 lr=0.000014777 loss=0.078 acc=0.990\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.84it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=58 loss=10.764 acc=0.766\n",
            " > Start epoch 59/80\n",
            "Epoch=59 Step=3425 lr=0.000014717 loss=0.121 acc=0.991\n",
            "Epoch=59 Step=3430 lr=0.000014656 loss=0.091 acc=0.989\n",
            "Epoch=59 Step=3435 lr=0.000014595 loss=0.060 acc=0.990\n",
            "Epoch=59 Step=3440 lr=0.000014535 loss=0.065 acc=0.990\n",
            "Epoch=59 Step=3445 lr=0.000014474 loss=0.085 acc=0.990\n",
            "Epoch=59 Step=3450 lr=0.000014414 loss=0.058 acc=0.990\n",
            "Epoch=59 Step=3455 lr=0.000014353 loss=0.090 acc=0.990\n",
            "Epoch=59 Step=3460 lr=0.000014293 loss=0.071 acc=0.990\n",
            "Epoch=59 Step=3465 lr=0.000014232 loss=0.064 acc=0.990\n",
            "Epoch=59 Step=3470 lr=0.000014172 loss=0.067 acc=0.990\n",
            "Epoch=59 Step=3475 lr=0.000014111 loss=0.076 acc=0.990\n",
            "Epoch=59 Step=3480 lr=0.000014050 loss=0.079 acc=0.990\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.80it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=59 loss=11.175 acc=0.758\n",
            " > Start epoch 60/80\n",
            "Epoch=60 Step=3485 lr=0.000013990 loss=0.067 acc=0.992\n",
            "Epoch=60 Step=3490 lr=0.000013929 loss=0.073 acc=0.991\n",
            "Epoch=60 Step=3495 lr=0.000013869 loss=0.082 acc=0.991\n",
            "Epoch=60 Step=3500 lr=0.000013808 loss=0.045 acc=0.991\n",
            "Epoch=60 Step=3505 lr=0.000013748 loss=0.066 acc=0.991\n",
            "Epoch=60 Step=3510 lr=0.000013687 loss=0.094 acc=0.991\n",
            "Epoch=60 Step=3515 lr=0.000013626 loss=0.062 acc=0.991\n",
            "Epoch=60 Step=3520 lr=0.000013566 loss=0.066 acc=0.991\n",
            "Epoch=60 Step=3525 lr=0.000013505 loss=0.065 acc=0.991\n",
            "Epoch=60 Step=3530 lr=0.000013445 loss=0.105 acc=0.991\n",
            "Epoch=60 Step=3535 lr=0.000013384 loss=0.097 acc=0.990\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.96it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=60 loss=11.267 acc=0.756\n",
            " > Start epoch 61/80\n",
            "Epoch=61 Step=3540 lr=0.000013324 loss=0.089 acc=0.989\n",
            "Epoch=61 Step=3545 lr=0.000013263 loss=0.098 acc=0.990\n",
            "Epoch=61 Step=3550 lr=0.000013203 loss=0.052 acc=0.991\n",
            "Epoch=61 Step=3555 lr=0.000013142 loss=0.092 acc=0.990\n",
            "Epoch=61 Step=3560 lr=0.000013081 loss=0.056 acc=0.991\n",
            "Epoch=61 Step=3565 lr=0.000013021 loss=0.096 acc=0.990\n",
            "Epoch=61 Step=3570 lr=0.000012960 loss=0.123 acc=0.990\n",
            "Epoch=61 Step=3575 lr=0.000012900 loss=0.073 acc=0.990\n",
            "Epoch=61 Step=3580 lr=0.000012839 loss=0.059 acc=0.990\n",
            "Epoch=61 Step=3585 lr=0.000012779 loss=0.062 acc=0.991\n",
            "Epoch=61 Step=3590 lr=0.000012718 loss=0.085 acc=0.991\n",
            "Epoch=61 Step=3595 lr=0.000012657 loss=0.078 acc=0.991\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.89it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=61 loss=11.196 acc=0.758\n",
            " > Start epoch 62/80\n",
            "Epoch=62 Step=3600 lr=0.000012597 loss=0.077 acc=0.993\n",
            "Epoch=62 Step=3605 lr=0.000012536 loss=0.051 acc=0.994\n",
            "Epoch=62 Step=3610 lr=0.000012476 loss=0.072 acc=0.993\n",
            "Epoch=62 Step=3615 lr=0.000012415 loss=0.053 acc=0.993\n",
            "Epoch=62 Step=3620 lr=0.000012355 loss=0.102 acc=0.992\n",
            "Epoch=62 Step=3625 lr=0.000012294 loss=0.110 acc=0.991\n",
            "Epoch=62 Step=3630 lr=0.000012234 loss=0.061 acc=0.991\n",
            "Epoch=62 Step=3635 lr=0.000012173 loss=0.068 acc=0.991\n",
            "Epoch=62 Step=3640 lr=0.000012112 loss=0.059 acc=0.991\n",
            "Epoch=62 Step=3645 lr=0.000012052 loss=0.064 acc=0.991\n",
            "Epoch=62 Step=3650 lr=0.000011991 loss=0.091 acc=0.991\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.96it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=62 loss=11.262 acc=0.761\n",
            " > Start epoch 63/80\n",
            "Epoch=63 Step=3655 lr=0.000011931 loss=0.086 acc=0.994\n",
            "Epoch=63 Step=3660 lr=0.000011870 loss=0.047 acc=0.993\n",
            "Epoch=63 Step=3665 lr=0.000011810 loss=0.085 acc=0.991\n",
            "Epoch=63 Step=3670 lr=0.000011749 loss=0.056 acc=0.992\n",
            "Epoch=63 Step=3675 lr=0.000011688 loss=0.055 acc=0.991\n",
            "Epoch=63 Step=3680 lr=0.000011628 loss=0.057 acc=0.992\n",
            "Epoch=63 Step=3685 lr=0.000011567 loss=0.075 acc=0.992\n",
            "Epoch=63 Step=3690 lr=0.000011507 loss=0.081 acc=0.991\n",
            "Epoch=63 Step=3695 lr=0.000011446 loss=0.067 acc=0.991\n",
            "Epoch=63 Step=3700 lr=0.000011386 loss=0.114 acc=0.991\n",
            "Epoch=63 Step=3705 lr=0.000011325 loss=0.040 acc=0.991\n",
            "Epoch=63 Step=3710 lr=0.000011265 loss=0.064 acc=0.991\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.00it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=63 loss=11.564 acc=0.756\n",
            " > Start epoch 64/80\n",
            "Epoch=64 Step=3715 lr=0.000011204 loss=0.070 acc=0.991\n",
            "Epoch=64 Step=3720 lr=0.000011143 loss=0.051 acc=0.992\n",
            "Epoch=64 Step=3725 lr=0.000011083 loss=0.079 acc=0.992\n",
            "Epoch=64 Step=3730 lr=0.000011022 loss=0.108 acc=0.990\n",
            "Epoch=64 Step=3735 lr=0.000010962 loss=0.057 acc=0.990\n",
            "Epoch=64 Step=3740 lr=0.000010901 loss=0.066 acc=0.990\n",
            "Epoch=64 Step=3745 lr=0.000010841 loss=0.088 acc=0.990\n",
            "Epoch=64 Step=3750 lr=0.000010780 loss=0.045 acc=0.991\n",
            "Epoch=64 Step=3755 lr=0.000010719 loss=0.038 acc=0.991\n",
            "Epoch=64 Step=3760 lr=0.000010659 loss=0.073 acc=0.991\n",
            "Epoch=64 Step=3765 lr=0.000010598 loss=0.072 acc=0.991\n",
            "Epoch=64 Step=3770 lr=0.000010538 loss=0.063 acc=0.991\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.01it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=64 loss=11.165 acc=0.765\n",
            " > Start epoch 65/80\n",
            "Epoch=65 Step=3775 lr=0.000010477 loss=0.049 acc=0.994\n",
            "Epoch=65 Step=3780 lr=0.000010417 loss=0.039 acc=0.994\n",
            "Epoch=65 Step=3785 lr=0.000010356 loss=0.055 acc=0.994\n",
            "Epoch=65 Step=3790 lr=0.000010296 loss=0.076 acc=0.992\n",
            "Epoch=65 Step=3795 lr=0.000010235 loss=0.049 acc=0.993\n",
            "Epoch=65 Step=3800 lr=0.000010174 loss=0.082 acc=0.992\n",
            "Epoch=65 Step=3805 lr=0.000010114 loss=0.061 acc=0.992\n",
            "Epoch=65 Step=3810 lr=0.000010053 loss=0.075 acc=0.992\n",
            "Epoch=65 Step=3815 lr=0.000009993 loss=0.068 acc=0.992\n",
            "Epoch=65 Step=3820 lr=0.000009932 loss=0.052 acc=0.992\n",
            "Epoch=65 Step=3825 lr=0.000009872 loss=0.081 acc=0.992\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.02it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=65 loss=11.447 acc=0.758\n",
            " > Start epoch 66/80\n",
            "Epoch=66 Step=3830 lr=0.000009811 loss=0.071 acc=0.996\n",
            "Epoch=66 Step=3835 lr=0.000009750 loss=0.065 acc=0.992\n",
            "Epoch=66 Step=3840 lr=0.000009690 loss=0.070 acc=0.992\n",
            "Epoch=66 Step=3845 lr=0.000009629 loss=0.049 acc=0.992\n",
            "Epoch=66 Step=3850 lr=0.000009569 loss=0.059 acc=0.992\n",
            "Epoch=66 Step=3855 lr=0.000009508 loss=0.063 acc=0.992\n",
            "Epoch=66 Step=3860 lr=0.000009448 loss=0.072 acc=0.992\n",
            "Epoch=66 Step=3865 lr=0.000009387 loss=0.052 acc=0.992\n",
            "Epoch=66 Step=3870 lr=0.000009327 loss=0.078 acc=0.991\n",
            "Epoch=66 Step=3875 lr=0.000009266 loss=0.087 acc=0.991\n",
            "Epoch=66 Step=3880 lr=0.000009205 loss=0.069 acc=0.991\n",
            "Epoch=66 Step=3885 lr=0.000009145 loss=0.061 acc=0.991\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.94it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=66 loss=11.450 acc=0.757\n",
            " > Start epoch 67/80\n",
            "Epoch=67 Step=3890 lr=0.000009084 loss=0.059 acc=0.991\n",
            "Epoch=67 Step=3895 lr=0.000009024 loss=0.031 acc=0.994\n",
            "Epoch=67 Step=3900 lr=0.000008963 loss=0.062 acc=0.993\n",
            "Epoch=67 Step=3905 lr=0.000008903 loss=0.119 acc=0.991\n",
            "Epoch=67 Step=3910 lr=0.000008842 loss=0.067 acc=0.991\n",
            "Epoch=67 Step=3915 lr=0.000008781 loss=0.088 acc=0.991\n",
            "Epoch=67 Step=3920 lr=0.000008721 loss=0.072 acc=0.991\n",
            "Epoch=67 Step=3925 lr=0.000008660 loss=0.068 acc=0.991\n",
            "Epoch=67 Step=3930 lr=0.000008600 loss=0.049 acc=0.992\n",
            "Epoch=67 Step=3935 lr=0.000008539 loss=0.063 acc=0.992\n",
            "Epoch=67 Step=3940 lr=0.000008479 loss=0.034 acc=0.992\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.82it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=67 loss=11.512 acc=0.761\n",
            " > Start epoch 68/80\n",
            "Epoch=68 Step=3945 lr=0.000008418 loss=0.056 acc=0.996\n",
            "Epoch=68 Step=3950 lr=0.000008358 loss=0.064 acc=0.994\n",
            "Epoch=68 Step=3955 lr=0.000008297 loss=0.065 acc=0.994\n",
            "Epoch=68 Step=3960 lr=0.000008236 loss=0.060 acc=0.993\n",
            "Epoch=68 Step=3965 lr=0.000008176 loss=0.052 acc=0.993\n",
            "Epoch=68 Step=3970 lr=0.000008115 loss=0.077 acc=0.992\n",
            "Epoch=68 Step=3975 lr=0.000008055 loss=0.063 acc=0.992\n",
            "Epoch=68 Step=3980 lr=0.000007994 loss=0.087 acc=0.993\n",
            "Epoch=68 Step=3985 lr=0.000007934 loss=0.071 acc=0.993\n",
            "Epoch=68 Step=3990 lr=0.000007873 loss=0.072 acc=0.992\n",
            "Epoch=68 Step=3995 lr=0.000007813 loss=0.056 acc=0.992\n",
            "Epoch=68 Step=4000 lr=0.000007752 loss=0.087 acc=0.992\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.95it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=68 loss=11.404 acc=0.762\n",
            " > Start epoch 69/80\n",
            "Epoch=69 Step=4005 lr=0.000007691 loss=0.065 acc=0.991\n",
            "Epoch=69 Step=4010 lr=0.000007631 loss=0.050 acc=0.993\n",
            "Epoch=69 Step=4015 lr=0.000007570 loss=0.090 acc=0.992\n",
            "Epoch=69 Step=4020 lr=0.000007510 loss=0.052 acc=0.993\n",
            "Epoch=69 Step=4025 lr=0.000007449 loss=0.068 acc=0.992\n",
            "Epoch=69 Step=4030 lr=0.000007389 loss=0.054 acc=0.992\n",
            "Epoch=69 Step=4035 lr=0.000007328 loss=0.060 acc=0.993\n",
            "Epoch=69 Step=4040 lr=0.000007267 loss=0.098 acc=0.992\n",
            "Epoch=69 Step=4045 lr=0.000007207 loss=0.046 acc=0.992\n",
            "Epoch=69 Step=4050 lr=0.000007146 loss=0.049 acc=0.992\n",
            "Epoch=69 Step=4055 lr=0.000007086 loss=0.069 acc=0.992\n",
            "Epoch=69 Step=4060 lr=0.000007025 loss=0.077 acc=0.992\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.92it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=69 loss=11.493 acc=0.759\n",
            " > Start epoch 70/80\n",
            "Epoch=70 Step=4065 lr=0.000006965 loss=0.064 acc=0.993\n",
            "Epoch=70 Step=4070 lr=0.000006904 loss=0.049 acc=0.993\n",
            "Epoch=70 Step=4075 lr=0.000006844 loss=0.071 acc=0.993\n",
            "Epoch=70 Step=4080 lr=0.000006783 loss=0.053 acc=0.993\n",
            "Epoch=70 Step=4085 lr=0.000006722 loss=0.060 acc=0.993\n",
            "Epoch=70 Step=4090 lr=0.000006662 loss=0.071 acc=0.992\n",
            "Epoch=70 Step=4095 lr=0.000006601 loss=0.095 acc=0.992\n",
            "Epoch=70 Step=4100 lr=0.000006541 loss=0.051 acc=0.992\n",
            "Epoch=70 Step=4105 lr=0.000006480 loss=0.041 acc=0.993\n",
            "Epoch=70 Step=4110 lr=0.000006420 loss=0.060 acc=0.992\n",
            "Epoch=70 Step=4115 lr=0.000006359 loss=0.067 acc=0.992\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.91it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=70 loss=11.582 acc=0.757\n",
            " > Start epoch 71/80\n",
            "Epoch=71 Step=4120 lr=0.000006298 loss=0.051 acc=0.995\n",
            "Epoch=71 Step=4125 lr=0.000006238 loss=0.087 acc=0.992\n",
            "Epoch=71 Step=4130 lr=0.000006177 loss=0.098 acc=0.990\n",
            "Epoch=71 Step=4135 lr=0.000006117 loss=0.075 acc=0.991\n",
            "Epoch=71 Step=4140 lr=0.000006056 loss=0.042 acc=0.992\n",
            "Epoch=71 Step=4145 lr=0.000005996 loss=0.031 acc=0.992\n",
            "Epoch=71 Step=4150 lr=0.000005935 loss=0.034 acc=0.993\n",
            "Epoch=71 Step=4155 lr=0.000005875 loss=0.092 acc=0.992\n",
            "Epoch=71 Step=4160 lr=0.000005814 loss=0.058 acc=0.992\n",
            "Epoch=71 Step=4165 lr=0.000005753 loss=0.079 acc=0.992\n",
            "Epoch=71 Step=4170 lr=0.000005693 loss=0.055 acc=0.992\n",
            "Epoch=71 Step=4175 lr=0.000005632 loss=0.054 acc=0.992\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.95it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=71 loss=11.754 acc=0.756\n",
            " > Start epoch 72/80\n",
            "Epoch=72 Step=4180 lr=0.000005572 loss=0.060 acc=0.992\n",
            "Epoch=72 Step=4185 lr=0.000005511 loss=0.072 acc=0.992\n",
            "Epoch=72 Step=4190 lr=0.000005451 loss=0.047 acc=0.993\n",
            "Epoch=72 Step=4195 lr=0.000005390 loss=0.045 acc=0.993\n",
            "Epoch=72 Step=4200 lr=0.000005329 loss=0.053 acc=0.993\n",
            "Epoch=72 Step=4205 lr=0.000005269 loss=0.044 acc=0.993\n",
            "Epoch=72 Step=4210 lr=0.000005208 loss=0.057 acc=0.993\n",
            "Epoch=72 Step=4215 lr=0.000005148 loss=0.078 acc=0.993\n",
            "Epoch=72 Step=4220 lr=0.000005087 loss=0.039 acc=0.993\n",
            "Epoch=72 Step=4225 lr=0.000005027 loss=0.050 acc=0.993\n",
            "Epoch=72 Step=4230 lr=0.000004966 loss=0.077 acc=0.993\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.87it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=72 loss=11.748 acc=0.756\n",
            " > Start epoch 73/80\n",
            "Epoch=73 Step=4235 lr=0.000004906 loss=0.054 acc=0.994\n",
            "Epoch=73 Step=4240 lr=0.000004845 loss=0.040 acc=0.995\n",
            "Epoch=73 Step=4245 lr=0.000004784 loss=0.080 acc=0.992\n",
            "Epoch=73 Step=4250 lr=0.000004724 loss=0.065 acc=0.993\n",
            "Epoch=73 Step=4255 lr=0.000004663 loss=0.061 acc=0.993\n",
            "Epoch=73 Step=4260 lr=0.000004603 loss=0.069 acc=0.992\n",
            "Epoch=73 Step=4265 lr=0.000004542 loss=0.042 acc=0.993\n",
            "Epoch=73 Step=4270 lr=0.000004482 loss=0.035 acc=0.993\n",
            "Epoch=73 Step=4275 lr=0.000004421 loss=0.091 acc=0.992\n",
            "Epoch=73 Step=4280 lr=0.000004360 loss=0.073 acc=0.992\n",
            "Epoch=73 Step=4285 lr=0.000004300 loss=0.052 acc=0.992\n",
            "Epoch=73 Step=4290 lr=0.000004239 loss=0.066 acc=0.992\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.87it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=73 loss=11.741 acc=0.760\n",
            " > Start epoch 74/80\n",
            "Epoch=74 Step=4295 lr=0.000004179 loss=0.059 acc=0.988\n",
            "Epoch=74 Step=4300 lr=0.000004118 loss=0.058 acc=0.991\n",
            "Epoch=74 Step=4305 lr=0.000004058 loss=0.036 acc=0.993\n",
            "Epoch=74 Step=4310 lr=0.000003997 loss=0.048 acc=0.993\n",
            "Epoch=74 Step=4315 lr=0.000003937 loss=0.071 acc=0.993\n",
            "Epoch=74 Step=4320 lr=0.000003876 loss=0.040 acc=0.993\n",
            "Epoch=74 Step=4325 lr=0.000003815 loss=0.074 acc=0.993\n",
            "Epoch=74 Step=4330 lr=0.000003755 loss=0.042 acc=0.993\n",
            "Epoch=74 Step=4335 lr=0.000003694 loss=0.075 acc=0.993\n",
            "Epoch=74 Step=4340 lr=0.000003634 loss=0.062 acc=0.993\n",
            "Epoch=74 Step=4345 lr=0.000003573 loss=0.045 acc=0.993\n",
            "Epoch=74 Step=4350 lr=0.000003513 loss=0.046 acc=0.993\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.87it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=74 loss=11.659 acc=0.763\n",
            " > Start epoch 75/80\n",
            "Epoch=75 Step=4355 lr=0.000003452 loss=0.057 acc=0.995\n",
            "Epoch=75 Step=4360 lr=0.000003391 loss=0.040 acc=0.995\n",
            "Epoch=75 Step=4365 lr=0.000003331 loss=0.032 acc=0.995\n",
            "Epoch=75 Step=4370 lr=0.000003270 loss=0.090 acc=0.994\n",
            "Epoch=75 Step=4375 lr=0.000003210 loss=0.055 acc=0.994\n",
            "Epoch=75 Step=4380 lr=0.000003149 loss=0.065 acc=0.994\n",
            "Epoch=75 Step=4385 lr=0.000003089 loss=0.083 acc=0.993\n",
            "Epoch=75 Step=4390 lr=0.000003028 loss=0.047 acc=0.993\n",
            "Epoch=75 Step=4395 lr=0.000002968 loss=0.084 acc=0.992\n",
            "Epoch=75 Step=4400 lr=0.000002907 loss=0.041 acc=0.993\n",
            "Epoch=75 Step=4405 lr=0.000002846 loss=0.044 acc=0.993\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.94it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=75 loss=11.743 acc=0.760\n",
            " > Start epoch 76/80\n",
            "Epoch=76 Step=4410 lr=0.000002786 loss=0.053 acc=0.995\n",
            "Epoch=76 Step=4415 lr=0.000002725 loss=0.048 acc=0.994\n",
            "Epoch=76 Step=4420 lr=0.000002665 loss=0.043 acc=0.995\n",
            "Epoch=76 Step=4425 lr=0.000002604 loss=0.058 acc=0.994\n",
            "Epoch=76 Step=4430 lr=0.000002544 loss=0.044 acc=0.994\n",
            "Epoch=76 Step=4435 lr=0.000002483 loss=0.043 acc=0.994\n",
            "Epoch=76 Step=4440 lr=0.000002422 loss=0.078 acc=0.993\n",
            "Epoch=76 Step=4445 lr=0.000002362 loss=0.064 acc=0.993\n",
            "Epoch=76 Step=4450 lr=0.000002301 loss=0.063 acc=0.993\n",
            "Epoch=76 Step=4455 lr=0.000002241 loss=0.078 acc=0.992\n",
            "Epoch=76 Step=4460 lr=0.000002180 loss=0.042 acc=0.993\n",
            "Epoch=76 Step=4465 lr=0.000002120 loss=0.052 acc=0.993\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.91it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.48s/it]\n",
            "Evaluation: Epoch=76 loss=11.875 acc=0.757\n",
            " > Start epoch 77/80\n",
            "Epoch=77 Step=4470 lr=0.000002059 loss=0.070 acc=0.994\n",
            "Epoch=77 Step=4475 lr=0.000001999 loss=0.029 acc=0.996\n",
            "Epoch=77 Step=4480 lr=0.000001938 loss=0.039 acc=0.996\n",
            "Epoch=77 Step=4485 lr=0.000001877 loss=0.046 acc=0.995\n",
            "Epoch=77 Step=4490 lr=0.000001817 loss=0.039 acc=0.995\n",
            "Epoch=77 Step=4495 lr=0.000001756 loss=0.053 acc=0.995\n",
            "Epoch=77 Step=4500 lr=0.000001696 loss=0.075 acc=0.994\n",
            "Epoch=77 Step=4505 lr=0.000001635 loss=0.048 acc=0.994\n",
            "Epoch=77 Step=4510 lr=0.000001575 loss=0.073 acc=0.993\n",
            "Epoch=77 Step=4515 lr=0.000001514 loss=0.044 acc=0.993\n",
            "Epoch=77 Step=4520 lr=0.000001453 loss=0.047 acc=0.993\n",
            "Batch: 100% 234/234 [00:47<00:00,  4.94it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=77 loss=11.737 acc=0.760\n",
            " > Start epoch 78/80\n",
            "Epoch=78 Step=4525 lr=0.000001393 loss=0.050 acc=0.993\n",
            "Epoch=78 Step=4530 lr=0.000001332 loss=0.070 acc=0.991\n",
            "Epoch=78 Step=4535 lr=0.000001272 loss=0.030 acc=0.994\n",
            "Epoch=78 Step=4540 lr=0.000001211 loss=0.067 acc=0.993\n",
            "Epoch=78 Step=4545 lr=0.000001151 loss=0.047 acc=0.993\n",
            "Epoch=78 Step=4550 lr=0.000001090 loss=0.067 acc=0.993\n",
            "Epoch=78 Step=4555 lr=0.000001030 loss=0.043 acc=0.993\n",
            "Epoch=78 Step=4560 lr=0.000000969 loss=0.036 acc=0.993\n",
            "Epoch=78 Step=4565 lr=0.000000908 loss=0.070 acc=0.993\n",
            "Epoch=78 Step=4570 lr=0.000000848 loss=0.040 acc=0.993\n",
            "Epoch=78 Step=4575 lr=0.000000787 loss=0.034 acc=0.994\n",
            "Epoch=78 Step=4580 lr=0.000000727 loss=0.089 acc=0.993\n",
            "Batch: 100% 234/234 [00:46<00:00,  5.01it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=78 loss=11.795 acc=0.759\n",
            " > Start epoch 79/80\n",
            "Epoch=79 Step=4585 lr=0.000000666 loss=0.036 acc=0.995\n",
            "Epoch=79 Step=4590 lr=0.000000606 loss=0.048 acc=0.995\n",
            "Epoch=79 Step=4595 lr=0.000000545 loss=0.046 acc=0.994\n",
            "Epoch=79 Step=4600 lr=0.000000484 loss=0.059 acc=0.994\n",
            "Epoch=79 Step=4605 lr=0.000000424 loss=0.050 acc=0.994\n",
            "Epoch=79 Step=4610 lr=0.000000363 loss=0.031 acc=0.994\n",
            "Epoch=79 Step=4615 lr=0.000000303 loss=0.025 acc=0.995\n",
            "Epoch=79 Step=4620 lr=0.000000242 loss=0.065 acc=0.995\n",
            "Epoch=79 Step=4625 lr=0.000000182 loss=0.101 acc=0.993\n",
            "Epoch=79 Step=4630 lr=0.000000121 loss=0.054 acc=0.993\n",
            "Epoch=79 Step=4635 lr=0.000000061 loss=0.069 acc=0.993\n",
            "Epoch=79 Step=4640 lr=0.000000000 loss=0.049 acc=0.993\n",
            "Batch: 100% 234/234 [00:48<00:00,  4.81it/s]\n",
            "Evaluating: 100% 10/10 [00:14<00:00,  1.47s/it]\n",
            "Evaluation: Epoch=79 loss=11.781 acc=0.760\n",
            " ➤ Loading data from test.tsv\n",
            "   dataset has 24 labels\n",
            "  0% 0/314 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100% 314/314 [00:01<00:00, 243.38it/s]\n",
            " ➤ Cached data in cache/getuigenissen-ner-bertje/test.tsv.pkl\n",
            "Exporting dev\n",
            "Evaluating: 100% 10/10 [00:15<00:00,  1.50s/it]\n",
            "Predictions are exported to output/getuigenissen-ner-bertje/dev.tsv\n",
            "Exporting test\n",
            "Evaluating: 100% 5/5 [00:07<00:00,  1.43s/it]\n",
            "Predictions are exported to output/getuigenissen-ner-bertje/test.tsv\n",
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE-UpCJtwx81"
      },
      "source": [
        "## RobBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTWZM2OzVqeg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ece7834-b81e-407a-835a-29c5addf997b"
      },
      "source": [
        "%cd /content\r\n",
        "!git clone https://github.com/iPieter/RobBERT\r\n",
        "%cd RobBERT\r\n",
        "#!git checkout v2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'RobBERT' already exists and is not an empty directory.\n",
            "/content/RobBERT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNXED8oWYTCr"
      },
      "source": [
        "#!pip install fairseq\r\n",
        "#!pip install nltk\r\n",
        "#!pip install numpy\r\n",
        "#!pip install torch==1.6.0\r\n",
        "#!pip install tokenizers==0.4.2\r\n",
        "#!pip install transformers==3.1.0\r\n",
        "#!pip install tensorboardx\r\n",
        "#!pip install nltk\r\n",
        "#!pip install pytorch-lightning\r\n",
        "#!pip install -e git+git@github.com:iPieter/kiwi.ml.git@76b66872fce68873809a0dea112e2ed552ae5b63#egg=kiwi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i8-3YJUzq7M"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iDi1ZogtXff"
      },
      "source": [
        "import transformers\r\n",
        "dir(transformers)\r\n",
        "from transformers import RobertaTokenizer, RobertaForTokenClassification\r\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\r\n",
        "model     = RobertaForTokenClassification.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9MwSaSZxlYT"
      },
      "source": [
        "txt = \"dag wereld\"\r\n",
        "print(tokenizer(txt)['input_ids'])\r\n",
        "inputs = tokenizer(txt, return_tensors = \"pt\")\r\n",
        "print(inputs)\r\n",
        "outputs = model(**inputs)\r\n",
        "print(outputs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}